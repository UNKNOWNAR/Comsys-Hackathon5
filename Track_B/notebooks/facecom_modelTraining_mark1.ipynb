{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12213659,"sourceType":"datasetVersion","datasetId":7694235}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === Config ===\nUSE_DCT_ATTENTION = True   # ðŸ”„ Controls whether to use attention maps based on DCT/FFT for training (can be toggled)\nIMG_SIZE = 224             # ðŸ”„ Defines the size (224x224) to which images will be resized for model input","metadata":{"_uuid":"0706ad79-4bd5-4bdc-adc4-169a5bf2d66b","_cell_guid":"75d17c6a-6a7c-4bff-9b74-9163c965cdfe","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\n\n\"\"\"\nthe function below just loads picks up the right paths from the test or the validation directory and\narranges them in a dictionary\nThe directory structure is that say we pickup a random directory X... under it we will have one or multiple clean images \nand under sub-dir named distortion under which all the distorted images are present\n\"\"\"\n# ðŸ“Œ Function to collect image paths from a directory structure, organizing them by person ID\ndef collect_image_paths(root_dir):\n    person_dict = {}  # ðŸ“Œ Dictionary to store paths: {person_id: {\"clean\": [paths], \"distorted\": [paths]}}\n\n    # ðŸ“Œ Iterate through each folder in the root directory (each folder represents a person)\n    for person_folder in sorted(os.listdir(root_dir)):\n        folder_path = os.path.join(root_dir, person_folder)\n        # ðŸ“Œ Skip if not a directory\n        if not os.path.isdir(folder_path):\n            continue\n\n        # ðŸ“Œ Collect all .jpg files directly in the person folder (clean images, excluding subfolders)\n        all_jpgs = glob.glob(os.path.join(folder_path, \"*.jpg\"))\n        clean_imgs = [f for f in all_jpgs if \"distortion\" not in f]\n\n        # ðŸ“Œ Collect distorted images from the 'distortion' subfolder\n        distortion_dir = os.path.join(folder_path, \"distortion\")\n        distortion_imgs = []\n        if os.path.exists(distortion_dir):\n            distortion_imgs = glob.glob(os.path.join(distortion_dir, \"*.jpg\"))\n\n        # ðŸ“Œ Only include person in dict if they have at least one clean image\n        if clean_imgs:\n            person_dict[person_folder] = {\n                \"clean\": clean_imgs,  # store list, not just one\n                \"distorted\": distortion_imgs\n            }\n\n    return person_dict","metadata":{"_uuid":"733518cf-6ad2-4499-aa22-9dfe3142dbe7","_cell_guid":"39e78b80-73f2-4e97-bd9b-ba7ff76e1206","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:23:50.607138Z","iopub.execute_input":"2025-07-01T02:23:50.607374Z","iopub.status.idle":"2025-07-01T02:23:50.619967Z","shell.execute_reply.started":"2025-07-01T02:23:50.607356Z","shell.execute_reply":"2025-07-01T02:23:50.619385Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Load training data from the specified directory\ntrain_dir = f\"/kaggle/input/facecom/Comys_Hackathon5/Task_B/train\"\nperson_dict = collect_image_paths(train_dir)  # ðŸ“Œ Create dictionary of image paths for training data","metadata":{"_uuid":"2ca3bdff-c5bf-4fcc-957c-292283fde85d","_cell_guid":"2e64908d-c616-4b80-8872-a9333ead09be","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:23:50.620566Z","iopub.execute_input":"2025-07-01T02:23:50.620756Z","iopub.status.idle":"2025-07-01T02:24:04.002563Z","shell.execute_reply.started":"2025-07-01T02:23:50.620741Z","shell.execute_reply":"2025-07-01T02:24:04.001975Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Load validation data from the specified directory\nvalidation_dir = f\"/kaggle/input/facecom/Comys_Hackathon5/Task_B/val\"\nval_dict = collect_image_paths(validation_dir)  # ðŸ“Œ Create dictionary of image paths for validation data","metadata":{"_uuid":"0632b589-cb3e-4390-8f3f-f2f08855aeac","_cell_guid":"72adebd3-e310-4a76-af51-a5591977f9d0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:54:35.936282Z","iopub.execute_input":"2025-07-01T02:54:35.936624Z","iopub.status.idle":"2025-07-01T02:54:40.892381Z","shell.execute_reply.started":"2025-07-01T02:54:35.936599Z","shell.execute_reply":"2025-07-01T02:54:40.891595Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Debugging: Print the structure of the person_dict to verify data loading\nprint(f\"Found {len(person_dict)} person folders\")\nprint(\"Available keys:\", list(person_dict.keys())[:5])  # ðŸ“Œ Show first 5 person IDs\n\n# FIX: Access dictionary correctly\nif person_dict:\n    # ðŸ“Œ Access the first person's data for inspection\n    first_person = list(person_dict.keys())[0]\n    print(f\"\\nFirst person '{first_person}':\")\n    print(person_dict[first_person])  # ðŸ“Œ Print clean and distorted image paths for the first person","metadata":{"_uuid":"9ef19b56-0dbd-4354-8a40-ed26d2b96653","_cell_guid":"a0ae938e-3ef4-454a-8e8e-596b5e3009dc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:04.003262Z","iopub.execute_input":"2025-07-01T02:24:04.003473Z","iopub.status.idle":"2025-07-01T02:24:04.008893Z","shell.execute_reply.started":"2025-07-01T02:24:04.003456Z","shell.execute_reply":"2025-07-01T02:24:04.008311Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# ðŸ“Œ Convert person_dict to a DataFrame for statistical analysis\nrecords = []\nfor identity, imgs in person_dict.items():\n    records.append({\n        \"identity\": identity,\n        \"num_clean\": len(imgs[\"clean\"]),  # ðŸ“Œ Count of clean images per person\n        \"num_distorted\": len(imgs[\"distorted\"])  # ðŸ“Œ Count of distorted images per person\n    })\n\ndf = pd.DataFrame(records)\n\n# ðŸ“Š Histogram: Distorted Images\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\"num_distorted\"], bins=30, color=\"skyblue\")\nplt.title(\"Number of Distorted Images per Identity\")\nplt.xlabel(\"Distorted Images\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n# ðŸ“Š Histogram: Undistorted Images\nplt.figure(figsize=(8, 5))\nsns.countplot(data=df, x=\"num_clean\", palette=\"Set2\")\nplt.title(\"Number of Undistorted Images per Identity\")\nplt.xlabel(\"Undistorted Images\")\nplt.ylabel(\"Number of Identities\")\nplt.grid(True, axis='y')\nplt.show()\n\n# ðŸ“‹ Basic Stats\nprint(f\"ðŸ“Œ Total Identities: {df.shape[0]}\")  # ðŸ“Œ Total number of unique persons\nprint(f\"ðŸ“¸ Avg distorted per identity: {df['num_distorted'].mean():.2f}\")  # ðŸ“Œ Average number of distorted images\nprint(f\"ðŸ§¼ Avg undistorted per identity: {df['num_clean'].mean():.2f}\")  # ðŸ“Œ Average number of clean images\nprint(f\"ðŸ§¼ Min undistorted: {df['num_clean'].min()}, Max undistorted: {df['num_clean'].max()}\")  # ðŸ“Œ Min/max clean images","metadata":{"_uuid":"cbecf083-3946-4629-a697-6982151ee2c3","_cell_guid":"dcf52fa1-f9be-4741-b5a8-d546941b3f29","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:04.010930Z","iopub.execute_input":"2025-07-01T02:24:04.011187Z","iopub.status.idle":"2025-07-01T02:24:05.538378Z","shell.execute_reply.started":"2025-07-01T02:24:04.011162Z","shell.execute_reply":"2025-07-01T02:24:05.537561Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Calculate total possible positive pairs from clean images (combinations of clean images per person)\ndf[\"pos_pairs\"] = df[\"num_clean\"] * (df[\"num_clean\"] - 1) // 2  # ðŸ“Œ Formula: n*(n-1)/2 for combinations\ntotal_positive_pairs = df[\"pos_pairs\"].sum()\n\nprint(f\"âœ… Total possible positive pairs from undistorted images: {total_positive_pairs}\")","metadata":{"_uuid":"50b2b575-d0a5-42be-a100-08935a317bce","_cell_guid":"fd95b2ce-7742-4623-b167-48756cc5bef8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:05.539220Z","iopub.execute_input":"2025-07-01T02:24:05.539571Z","iopub.status.idle":"2025-07-01T02:24:05.545726Z","shell.execute_reply.started":"2025-07-01T02:24:05.539551Z","shell.execute_reply":"2025-07-01T02:24:05.544909Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# ðŸ“Œ Repeat the same analysis for validation data\nrecords_val = []\nfor identity, imgs in val_dict.items():\n    records_val.append({\n        \"identity\": identity,\n        \"num_clean\": len(imgs[\"clean\"]),\n        \"num_distorted\": len(imgs[\"distorted\"])\n    })\n\ndf = pd.DataFrame(records_val)\n\n# ðŸ“Š Histogram: Distorted Images\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\"num_distorted\"], bins=30, color=\"skyblue\")\nplt.title(\"Number of Distorted Images per Identity\")\nplt.xlabel(\"Distorted Images\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n# ðŸ“Š Histogram: Undistorted Images\nplt.figure(figsize=(8, 5))\nsns.countplot(data=df, x=\"num_clean\", palette=\"Set2\")\nplt.title(\"Number of Undistorted Images per Identity\")\nplt.xlabel(\"Undistorted Images\")\nplt.ylabel(\"Number of Identities\")\nplt.grid(True, axis='y')\nplt.show()\n\n# ðŸ“‹ Basic Stats\nprint(f\"ðŸ“Œ Total Identities: {df.shape[0]}\")\nprint(f\"ðŸ“¸ Avg distorted per identity: {df['num_distorted'].mean():.2f}\")\nprint(f\"ðŸ§¼ Avg undistorted per identity: {df['num_clean'].mean():.2f}\")\nprint(f\"ðŸ§¼ Min undistorted: {df['num_clean'].min()}, Max undistorted: {df['num_clean'].max()}\")","metadata":{"_uuid":"b2d440d3-3ec9-40b9-8f86-975c00129120","_cell_guid":"416a1ee0-e8d0-46b8-9f02-1cb90ce397ce","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:55:33.193178Z","iopub.execute_input":"2025-07-01T02:55:33.193864Z","iopub.status.idle":"2025-07-01T02:55:33.601294Z","shell.execute_reply.started":"2025-07-01T02:55:33.193839Z","shell.execute_reply":"2025-07-01T02:55:33.599943Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\n# ðŸ“Œ Function to generate balanced positive and negative pairs for face verification\ndef generate_balanced_augmented_pairs(person_dict, min_pos_per_id=28, num_neg_per_pos=3, seed=42):\n    \"\"\"\n    Generates positive and negative pairs for face verification task.\n    \n    - Ensures at least `min_pos_per_id` positive pairs per identity (if possible).\n    - For each positive, generates `num_neg_per_pos` negative pairs from other identities.\n    - Random sampling ensures balance and coverage of identities.\n\n    Args:\n        person_dict (dict): Dictionary with keys as person IDs and values as dicts with 'clean' and 'distorted' paths.\n        min_pos_per_id (int): Minimum number of positive pairs per identity.\n        num_neg_per_pos (int): Number of negative pairs to generate per positive pair.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        List of (img1_path, img2_path, label) tuples.\n    \"\"\"\n    random.seed(seed)  # ðŸ“Œ Set seed for reproducible random sampling\n    all_ids = list(person_dict.keys())  # ðŸ“Œ List of all person IDs\n    positive_pairs = []  # ðŸ“Œ Store positive pairs (same person)\n    negative_pairs = []  # ðŸ“Œ Store negative pairs (different people)\n\n    for person_id in all_ids:\n        images = []\n        # ðŸ“Œ Collect all images (clean and distorted) for the current person\n        if isinstance(person_dict[person_id]['clean'], list):\n            images += person_dict[person_id]['clean']\n        elif person_dict[person_id]['clean']:\n            images.append(person_dict[person_id]['clean'])\n\n        images += person_dict[person_id]['distorted']\n\n        # ðŸ“Œ Skip if fewer than 2 images (can't form a pair)\n        if len(images) < 2:\n            continue\n\n        # ðŸ“Œ Generate all possible positive pairs (combinations of 2 images)\n        all_pos_pairs = [(a, b) for idx, a in enumerate(images) for b in images[idx+1:] if a != b]\n\n        # ðŸ“Œ Sample up to min_pos_per_id pairs (or fewer if not enough)\n        selected_pos_pairs = random.sample(all_pos_pairs, min(min_pos_per_id, len(all_pos_pairs)))\n\n        for img1, img2 in selected_pos_pairs:\n            positive_pairs.append((img1, img2, 1))  # ðŸ“Œ Label 1 for positive pair\n\n            # ðŸ“Œ Generate negative pairs for each positive pair\n            other_ids = [pid for pid in all_ids if pid != person_id]\n            sampled_neg_ids = random.sample(other_ids, min(num_neg_per_pos, len(other_ids)))\n\n            for neg_id in sampled_neg_ids:\n                neg_candidates = person_dict[neg_id]['distorted'] or [person_dict[neg_id]['clean']]\n                if not neg_candidates:\n                    continue\n                neg_img = random.choice(neg_candidates)  # ðŸ“Œ Randomly select a negative image\n                negative_pairs.append((img1, neg_img, 0))  # ðŸ“Œ Label 0 for negative pair\n\n    # ðŸ“Œ Combine and shuffle all pairs\n    all_pairs = positive_pairs + negative_pairs\n    random.shuffle(all_pairs)\n    return all_pairs, positive_pairs, negative_pairs","metadata":{"_uuid":"1c68f43d-1bc9-4018-9b3f-35c0941a9648","_cell_guid":"8b0bc21a-014c-4b4a-b299-e90fc3e1837f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:57:22.598896Z","iopub.execute_input":"2025-07-01T02:57:22.599188Z","iopub.status.idle":"2025-07-01T02:57:22.608741Z","shell.execute_reply.started":"2025-07-01T02:57:22.599167Z","shell.execute_reply":"2025-07-01T02:57:22.607889Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Generate pairs for training data\nall_pairs, positive_pairs, negative_pairs = generate_balanced_augmented_pairs(\n    person_dict=person_dict, \n    min_pos_per_id=28, \n    num_neg_per_pos=1\n)","metadata":{"_uuid":"0ca59cb3-0a1a-45d3-bfba-eb399027ee72","_cell_guid":"e729c7bf-a51c-44e5-b88d-05196c357a62","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:05.562501Z","iopub.execute_input":"2025-07-01T02:24:05.562938Z","iopub.status.idle":"2025-07-01T02:24:06.530917Z","shell.execute_reply.started":"2025-07-01T02:24:05.562923Z","shell.execute_reply":"2025-07-01T02:24:06.530365Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Generate pairs for validation data\nall_val_pairs, val_pos , val_neg = generate_balanced_augmented_pairs(val_dict,20,1)","metadata":{"_uuid":"272b7bc3-a5c8-4268-9ddf-cc0f01d3eb0a","_cell_guid":"7328e4b2-c77b-4115-b269-6984328f25b5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:57:33.682341Z","iopub.execute_input":"2025-07-01T02:57:33.682598Z","iopub.status.idle":"2025-07-01T02:57:33.745727Z","shell.execute_reply.started":"2025-07-01T02:57:33.682581Z","shell.execute_reply":"2025-07-01T02:57:33.745238Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Print statistics about the generated pairs\nprint(f\"âœ… Total pairs: {len(all_pairs)}\")\nprint(f\"ðŸ”µ Positive pairs: {len(positive_pairs)}\")\nprint(f\"ðŸ”´ Negative pairs: {len(negative_pairs)}\")\nprint(\"ðŸ§¾ Sample pairs:\", all_pairs[:3])\n\n# ðŸ“Œ Visualize distribution of positive vs negative pairs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\npair_stats = {\"identity\": [], \"label\": []}\nfor img1, img2, label in all_pairs:\n    identity = os.path.basename(os.path.dirname(img1))\n    pair_stats[\"identity\"].append(identity)\n    pair_stats[\"label\"].append(label)\n\ndf_pairs = pd.DataFrame(pair_stats)\n\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df_pairs, x=\"label\", palette=\"Set2\")\nplt.xticks([0, 1], [\"Negative\", \"Positive\"])\nplt.title(\"Distribution of Positive vs Negative Pairs\")\nplt.xlabel(\"Pair Type\")\nplt.ylabel(\"Count\")\nplt.grid(True, axis='y')\nplt.show()","metadata":{"_uuid":"4e9e70ba-3eb5-440d-b56a-93db6051cdfd","_cell_guid":"87743182-64fa-4232-b210-5cdea3f7b7e6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:06.531578Z","iopub.execute_input":"2025-07-01T02:24:06.531744Z","iopub.status.idle":"2025-07-01T02:24:06.726892Z","shell.execute_reply.started":"2025-07-01T02:24:06.531731Z","shell.execute_reply":"2025-07-01T02:24:06.726240Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Repeat pair statistics and visualization for validation data\nprint(f\"âœ… Total pairs: {len(all_val_pairs)}\")\nprint(f\"ðŸ”µ Positive pairs: {len(val_pos)}\")\nprint(f\"ðŸ”´ Negative pairs: {len(val_neg)}\")\nprint(\"ðŸ§¾ Sample pairs:\", all_pairs[:3])\n\npair_stats = {\"identity\": [], \"label\": []}\nfor img1, img2, label in all_val_pairs:\n    identity = os.path.basename(os.path.dirname(img1))\n    pair_stats[\"identity\"].append(identity)\n    pair_stats[\"label\"].append(label)\n\ndf_pairs = pd.DataFrame(pair_stats)\n\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df_pairs, x=\"label\", palette=\"Set2\")\nplt.xticks([0, 1], [\"Negative\", \"Positive\"])\nplt.title(\"Distribution of Positive vs Negative Pairs\")\nplt.xlabel(\"Pair Type\")\nplt.ylabel(\"Count\")\nplt.grid(True, axis='y')\nplt.show()","metadata":{"_uuid":"af9185a4-939c-49d7-b59c-d5be0751f496","_cell_guid":"6bf2a22b-f9d0-45b2-b534-002399a89ab9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:58:19.950160Z","iopub.execute_input":"2025-07-01T02:58:19.950423Z","iopub.status.idle":"2025-07-01T02:58:20.086048Z","shell.execute_reply.started":"2025-07-01T02:58:19.950404Z","shell.execute_reply":"2025-07-01T02:58:20.085378Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\n# ðŸ“Œ Function to display positive and negative image pairs for visual inspection\ndef show_image_pairs(pairs, title, num=5):\n    plt.figure(figsize=(15, 3 * num))\n    for i in range(num):\n        img1 = Image.open(pairs[i][0])\n        img2 = Image.open(pairs[i][1])\n        \n        # ðŸ“Œ Display first image of the pair\n        plt.subplot(num, 2, 2 * i + 1)\n        plt.imshow(img1)\n        plt.axis(\"off\")\n        plt.title(f\"{title} Pair {i+1} - Img1\")\n\n        # ðŸ“Œ Display second image of the pair\n        plt.subplot(num, 2, 2 * i + 2)\n        plt.imshow(img2)\n        plt.axis(\"off\")\n        plt.title(f\"{title} Pair {i+1} - Img2\")\n\n    plt.tight_layout()\n    plt.show()\n\n# ðŸ“Œ Show 5 positive and 5 negative pairs for visual verification\nshow_image_pairs(positive_pairs, \"Positive\", num=5)\nshow_image_pairs(negative_pairs, \"Negative\", num=5)","metadata":{"_uuid":"c6a3fa0e-08dc-4093-8dbf-2a4dd1e13436","_cell_guid":"39ab76b5-11c6-430d-998f-d0cb6f1a5ae4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:06.727639Z","iopub.execute_input":"2025-07-01T02:24:06.727907Z","iopub.status.idle":"2025-07-01T02:24:19.605976Z","shell.execute_reply.started":"2025-07-01T02:24:06.727883Z","shell.execute_reply":"2025-07-01T02:24:19.605078Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as T\nfrom PIL import Image\n\n# ðŸ“Œ Define transformation pipeline for converting images to tensors\nto_tensor = T.Compose([\n    T.Grayscale(),  # ðŸ“Œ Convert images to grayscale\n    T.Resize((224, 224)),  # ðŸ“Œ Resize to 224x224\n    T.ToTensor()  # ðŸ“Œ Convert to PyTorch tensor\n])\n\n# ðŸ“Œ Function to compute FFT-based attention maps for a batch of image pairs\ndef compute_fft_attention_batch(batch1, batch2):\n    f1 = torch.fft.fft2(batch1)             # ðŸ“Œ Compute 2D FFT on first batch of images\n    f2 = torch.fft.fft2(batch2)             # ðŸ“Œ Compute 2D FFT on second batch of images\n    diff = torch.abs(f1 - f2)               # ðŸ“Œ Compute absolute difference in frequency domain\n    attn_maps = torch.fft.ifft2(diff).real  # ðŸ“Œ Inverse FFT to get attention map\n    attn_maps -= attn_maps.amin(dim=(1, 2), keepdim=True)  # ðŸ“Œ Normalize: subtract min\n    attn_maps /= (attn_maps.amax(dim=(1, 2), keepdim=True) + 1e-8)  # ðŸ“Œ Normalize: divide by max\n    attn_maps = 1.0 - attn_maps  # ðŸ“Œ Invert attention map\n    return attn_maps.unsqueeze(1)  # ðŸ“Œ Add channel dimension: [B, 1, H, W]","metadata":{"_uuid":"b60cf4d7-7162-4270-89d3-484b851eade0","_cell_guid":"9e07596a-8710-49c1-b115-c09ede9ba5b9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:19.606869Z","iopub.execute_input":"2025-07-01T02:24:19.607141Z","iopub.status.idle":"2025-07-01T02:24:27.518783Z","shell.execute_reply.started":"2025-07-01T02:24:19.607123Z","shell.execute_reply":"2025-07-01T02:24:27.518239Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, hashlib\nfrom torchvision.transforms import Compose, Grayscale, Resize, ToTensor\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport torch\n\n# ðŸ“Œ Set device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ðŸ“Œ Define paths and transformations for attention map caching\nresize = Resize((224, 224))\nattention_root = \"/kaggle/working/fft_attention_maps\"\nos.makedirs(attention_root, exist_ok=True)  # ðŸ“Œ Create directory for caching attention maps\n\nto_tensor = Compose([Grayscale(), resize, ToTensor()])","metadata":{"_uuid":"c62afd98-4465-43b6-8cb4-62e929b7499a","_cell_guid":"75412dd8-5145-447f-823d-860c460f6abe","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:27.519467Z","iopub.execute_input":"2025-07-01T02:24:27.519790Z","iopub.status.idle":"2025-07-01T02:24:27.604797Z","shell.execute_reply.started":"2025-07-01T02:24:27.519774Z","shell.execute_reply":"2025-07-01T02:24:27.604081Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Generate a unique cache key for each image pair using MD5 hash\ndef _cache_key(path1, path2):\n    a, b = sorted([os.path.abspath(path1), os.path.abspath(path2)])\n    key = hashlib.md5(f\"{a}|{b}\".encode()).hexdigest()\n    return os.path.join(attention_root, f\"{key}.pt\")","metadata":{"_uuid":"f49bee29-b318-4962-aae1-4e08f6455afe","_cell_guid":"0ca46601-cef1-475e-81e9-25a3d8462083","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:27.605580Z","iopub.execute_input":"2025-07-01T02:24:27.605822Z","iopub.status.idle":"2025-07-01T02:24:27.609782Z","shell.execute_reply.started":"2025-07-01T02:24:27.605801Z","shell.execute_reply":"2025-07-01T02:24:27.609025Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Function to compute and cache FFT attention maps for image pairs in batches\ndef batch_cache_attention(pairs, batch_size=32):\n    uncached = []\n    paths = []\n\n    # ðŸ“Œ Identify pairs that need attention maps computed (not already cached)\n    for p1, p2, _ in pairs:\n        out_path = _cache_key(p1, p2)\n        if not os.path.exists(out_path):\n            uncached.append((p1, p2))\n            paths.append(out_path)\n\n    if not uncached:\n        print(\"âœ… All attention maps already cached.\")\n        return\n\n    # ðŸ“Œ Process pairs in batches\n    for i in tqdm(range(0, len(uncached), batch_size), desc=\"âš¡ Batch-caching attention maps\"):\n        batch = uncached[i:i + batch_size]\n        batch_paths = paths[i:i + batch_size]\n\n        imgs1 = []\n        imgs2 = []\n\n        # ðŸ“Œ Load and preprocess images\n        for p1, p2 in batch:\n            try:\n                imgs1.append(to_tensor(Image.open(p1).convert(\"RGB\")))\n                imgs2.append(to_tensor(Image.open(p2).convert(\"RGB\")))\n            except Exception as e:\n                print(f\"âŒ Failed to load: {p1} or {p2} â€” {e}\")\n\n        if not imgs1 or not imgs2:\n            continue\n\n        t1 = torch.stack(imgs1).squeeze(1).to(device)  # ðŸ“Œ Stack images into batch: [B, H, W]\n        t2 = torch.stack(imgs2).squeeze(1).to(device)\n\n        attn_batch = compute_fft_attention_batch(t1, t2).cpu()  # ðŸ“Œ Compute attention maps\n\n        # ðŸ“Œ Save each attention map to disk\n        for attn_map, out_path in zip(attn_batch, batch_paths):\n            torch.save(attn_map.half(), out_path, pickle_protocol=4)","metadata":{"_uuid":"55390869-c2ed-4a41-a918-f9ec13b3983b","_cell_guid":"f202507b-daa0-423d-9d5b-af3f5871c8be","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:27.612064Z","iopub.execute_input":"2025-07-01T02:24:27.612286Z","iopub.status.idle":"2025-07-01T02:24:27.624360Z","shell.execute_reply.started":"2025-07-01T02:24:27.612271Z","shell.execute_reply":"2025-07-01T02:24:27.623672Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Cache attention maps for training pairs if enabled\nif USE_DCT_ATTENTION:\n    batch_cache_attention(all_pairs, batch_size=32)\nelse:\n    print(\"âš¡ Skipping FFT-attention caching (USE_DCT_ATTENTION = False)\")","metadata":{"_uuid":"340790ac-d120-4de0-927c-1dbc1c38fc99","_cell_guid":"06e285a8-399a-4654-a1de-e6c7a163d8dd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:24:27.624987Z","iopub.execute_input":"2025-07-01T02:24:27.625265Z","iopub.status.idle":"2025-07-01T02:41:50.265588Z","shell.execute_reply.started":"2025-07-01T02:24:27.625240Z","shell.execute_reply":"2025-07-01T02:41:50.264706Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Cache attention maps for validation pairs if enabled\nif USE_DCT_ATTENTION:\n    batch_cache_attention(all_val_pairs, batch_size=32)\nelse:\n    print(\"âš¡ Skipping FFT-attention caching (USE_DCT_ATTENTION = False)\")","metadata":{"_uuid":"8b9546f7-6f06-4084-a3f9-fa3076aca0de","_cell_guid":"b8c153e0-91e5-436e-a7b6-1462c0557cb7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:59:09.940089Z","iopub.execute_input":"2025-07-01T02:59:09.940624Z","iopub.status.idle":"2025-07-01T03:02:53.967693Z","shell.execute_reply.started":"2025-07-01T02:59:09.940602Z","shell.execute_reply":"2025-07-01T03:02:53.967135Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport os\n\n# ðŸ“Œ Load and visualize a sample attention map\nattn_path = \"/kaggle/working/fft_attention_maps/08a14f70e2e319f02c48cb5d13f9e37b.pt\"\nattn_tensor = torch.load(attn_path, weights_only=False).squeeze()  # ðŸ“Œ Load and remove channel dimension\n\nplt.figure(figsize=(6, 6))\nplt.imshow(attn_tensor.cpu().numpy(), cmap='viridis')  # ðŸ“Œ Display as heatmap\nplt.colorbar(label=\"Attention Intensity\")\nplt.title(\"DCT/FFT-based Attention Map\")\nplt.axis(\"off\")\nplt.show()","metadata":{"_uuid":"cced6e90-a3ef-4e24-93f0-213eaa58bddd","_cell_guid":"2254e5e1-cba0-4728-8558-258351e64ca5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:41:50.266475Z","iopub.execute_input":"2025-07-01T02:41:50.266770Z","iopub.status.idle":"2025-07-01T02:41:50.435896Z","shell.execute_reply.started":"2025-07-01T02:41:50.266746Z","shell.execute_reply":"2025-07-01T02:41:50.435227Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport hashlib\n\n# ðŸ“Œ Function to find original image paths from a cached attention map\ndef reverse_lookup_path(attn_path, all_pairs):\n    target_hash = os.path.basename(attn_path).replace(\".pt\", \"\")\n\n    for path1, path2, _ in all_pairs:\n        a, b = sorted([os.path.abspath(path1), os.path.abspath(path2)])\n        key_raw = f\"{a}|{b}\"\n        key_hash = hashlib.md5(key_raw.encode()).hexdigest()\n        if key_hash == target_hash:\n            return path1, path2\n    return None, None\n\nfrom PIL import Image\nimport numpy as np\nfrom scipy.fftpack import dct, idct\nimport torch\n\n# ðŸ“Œ Compute 2D DCT for an image\ndef apply_2d_dct(img):\n    return dct(dct(img.T, norm='ortho').T, norm='ortho')\n\n# ðŸ“Œ Compute 2D inverse DCT\ndef apply_2d_idct(coeffs):\n    return idct(idct(coeffs.T, norm='ortho').T, norm='ortho')\n\n# ðŸ“Œ Compute DCT-based attention map for a pair of images\ndef compute_dct_attention(img1, img2):\n    img1_np = np.array(img1.convert(\"L\"), dtype=np.float32)\n    img2_np = np.array(img2.convert(\"L\"), dtype=np.float32)\n    dct1 = apply_2d_dct(img1_np)\n    dct2 = apply_2d_dct(img2_np)\n    diff = np.abs(dct1 - dct2)\n    attn_map = apply_2d_idct(diff)\n    attn_map -= attn_map.min()\n    attn_map /= (attn_map.max() + 1e-8)\n    attn_map = 1.0 - attn_map\n    return torch.tensor(attn_map).float()","metadata":{"_uuid":"7c5e00ba-7a9e-4caf-a59d-53140ccf4c22","_cell_guid":"00e48b7c-8f7a-4882-be29-0983144f7925","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:41:50.436657Z","iopub.execute_input":"2025-07-01T02:41:50.436856Z","iopub.status.idle":"2025-07-01T02:41:50.458337Z","shell.execute_reply.started":"2025-07-01T02:41:50.436832Z","shell.execute_reply":"2025-07-01T02:41:50.457644Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# ðŸ“Œ Load cached attention map for comparison\nattn_cached = torch.load(attn_path, weights_only=False).squeeze()\n\n# ðŸ“Œ Find original images for the attention map\nimg1_path, img2_path = reverse_lookup_path(attn_path, all_pairs)\nassert img1_path and img2_path, \"âŒ Original paths not found!\"\n\n# ðŸ“Œ Recompute DCT attention map\nimg1 = Image.open(img1_path).convert(\"RGB\")\nimg2 = Image.open(img2_path).convert(\"RGB\")\nattn_dct = compute_dct_attention(img1, img2)\n\n# ðŸ“Œ Resize if shapes don't match\nif attn_cached.shape != attn_dct.shape:\n    from torchvision.transforms import Resize\n    attn_dct = Resize(attn_cached.shape)(attn_dct.unsqueeze(0)).squeeze(0)\n\n# ðŸ“Œ Visualize cached vs recomputed attention maps\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(attn_cached.cpu().numpy(), cmap='viridis')\nplt.title(\"Cached Attention Map\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(attn_dct.cpu().numpy(), cmap='viridis')\nplt.title(\"Recomputed DCT Attention\")\nplt.axis(\"off\")\n\nplt.suptitle(\"Sanity Check: Cached vs DCT Recomputed\", fontsize=14)\nplt.show()","metadata":{"_uuid":"45eb74a1-4c1d-4fa9-acf1-c5968043ef86","_cell_guid":"82590545-bce9-4430-a77b-7b3ce3b3527c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T02:41:50.459027Z","iopub.execute_input":"2025-07-01T02:41:50.459436Z","iopub.status.idle":"2025-07-01T02:41:50.676943Z","shell.execute_reply.started":"2025-07-01T02:41:50.459407Z","shell.execute_reply":"2025-07-01T02:41:50.676268Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, hashlib\nfrom torchvision.transforms import Compose, Grayscale, Resize, ToTensor\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport torch\n\n# ðŸ“Œ Reinitialize device and attention map caching setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresize = Resize((224, 224))\nattention_root = \"/kaggle/working/fft_attention_maps\"\nos.makedirs(attention_root, exist_ok=True)\nto_tensor = Compose([Grayscale(), resize, ToTensor()])","metadata":{"_uuid":"01947d22-f557-4770-808c-9ff3421a3ace","_cell_guid":"3fc9a959-7900-4a4a-8d71-1077c288897d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T03:06:35.516982Z","iopub.execute_input":"2025-07-01T03:06:35.517310Z","iopub.status.idle":"2025-07-01T03:06:35.522584Z","shell.execute_reply.started":"2025-07-01T03:06:35.517291Z","shell.execute_reply":"2025-07-01T03:06:35.521827Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\n# ðŸ“Œ Custom Dataset class for loading image pairs and their attention maps\nclass FacePairDataset(Dataset):\n    def __init__(self, pairs, transform=None):\n        self.pairs = pairs\n        self.transform = transform or T.Compose([\n            T.Resize((IMG_SIZE, IMG_SIZE)), T.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        p1, p2, label = self.pairs[idx]\n        img1 = Image.open(p1).convert(\"RGB\")\n        img2 = Image.open(p2).convert(\"RGB\")\n        img1_t = self.transform(img1)\n        img2_t = self.transform(img2)\n        label_t = torch.tensor(label, dtype=torch.float32)\n\n        if USE_DCT_ATTENTION:\n            attn = torch.load(_cache_key(p1, p2), weights_only=False)\n            return {\"img1\": img1_t, \"img2\": img2_t, \"attn\": attn, \"label\": label_t}\n        else:\n            return {\"img1\": img1_t, \"img2\": img2_t, \"label\": label_t}","metadata":{"_uuid":"6bbe597a-83db-4ee2-8c4c-7e997fd59ed0","_cell_guid":"122b3d28-61fa-4ab5-ba87-1fa358767fe9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T03:08:41.609421Z","iopub.execute_input":"2025-07-01T03:08:41.609705Z","iopub.status.idle":"2025-07-01T03:08:41.615747Z","shell.execute_reply.started":"2025-07-01T03:08:41.609676Z","shell.execute_reply":"2025-07-01T03:08:41.614884Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Create training dataset and inspect a sample\ntrain_dataset = FacePairDataset(all_pairs)\nprint(\"Total samples:\", len(train_dataset))\n\nsample = train_dataset[0]\nimg1  = sample[\"img1\"]\nimg2  = sample[\"img2\"]\nattn  = sample.get(\"attn\", None)  # ðŸ“Œ Safe access in case attention is disabled\nlabel = sample[\"label\"]\n\nprint(f\"Image 1 shape       : {img1.shape}\")\nprint(f\"Image 2 shape       : {img2.shape}\")\nif attn is not None:\n    print(f\"Attention map shape : {attn.shape}\")\nelse:\n    print(\"Attention map       : âŒ Not used (USE_DCT_ATTENTION = False)\")\nprint(f\"Label               : {label}\")","metadata":{"_uuid":"e0c0e78b-5008-40d9-b924-2f955f508fc2","_cell_guid":"61eb65e1-869b-4b93-b0c3-77865d039010","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T03:08:44.211767Z","iopub.execute_input":"2025-07-01T03:08:44.212495Z","iopub.status.idle":"2025-07-01T03:08:44.225142Z","shell.execute_reply.started":"2025-07-01T03:08:44.212470Z","shell.execute_reply":"2025-07-01T03:08:44.224357Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Create validation dataset and inspect a sample\nval_dataset = FacePairDataset(all_val_pairs)\nprint(\"Total samples:\", len(val_dataset))\n\nsample = val_dataset[0]\nimg1  = sample[\"img1\"]\nimg2  = sample[\"img2\"]\nattn  = sample.get(\"attn\", None)\nlabel = sample[\"label\"]\n\nprint(f\"Image 1 shape       : {img1.shape}\")\nprint(f\"Image 2 shape       : {img2.shape}\")\nif attn is not None:\n    print(f\"Attention map shape : {attn.shape}\")\nelse:\n    print(\"Attention map       : âŒ Not used (USE_DCT_ATTENTION = False)\")\nprint(f\"Label               : {label}\")","metadata":{"_uuid":"7e2d1fb6-e235-4465-845e-d6430364a768","_cell_guid":"ddcf390c-7369-4e34-9d77-b11378f82f62","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T03:09:20.980434Z","iopub.execute_input":"2025-07-01T03:09:20.980905Z","iopub.status.idle":"2025-07-01T03:09:21.154369Z","shell.execute_reply.started":"2025-07-01T03:09:20.980883Z","shell.execute_reply":"2025-07-01T03:09:21.153607Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# ðŸ“Œ Define Siamese Network for face verification\nclass SiameseNet(nn.Module):\n    def __init__(self, backbone=\"resnet18\", pretrained=True):\n        super().__init__()\n\n        # ðŸ“Œ Load pre-trained backbone (e.g., ResNet18) and remove the final layer\n        base = getattr(models, backbone)(pretrained=pretrained)\n        self.feature_extractor = nn.Sequential(*list(base.children())[:-1])\n\n        # ðŸ“Œ Define fully connected head for classification\n        self.fc = nn.Sequential(\n            nn.Linear(512 * 2, 512),  # ðŸ“Œ Concatenate features from both images\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 1)  # ðŸ“Œ Output a single logit for binary classification\n        )\n\n    def forward(self, img1, img2, attn_map=None):\n        \"\"\"\n        img1, img2: [B, 3, H, W]\n        attn_map   : [B, 1, H, W] or None\n        \"\"\"\n        # ðŸ“Œ Apply attention map to images if enabled\n        if USE_DCT_ATTENTION and attn_map is not None:\n            attn_map = attn_map.expand(-1, 3, -1, -1)  # ðŸ“Œ Broadcast to RGB channels\n            img1, img2 = img1 * attn_map, img2 * attn_map\n\n        # ðŸ“Œ Extract features for both images\n        f1 = self.feature_extractor(img1).view(img1.size(0), -1)\n        f2 = self.feature_extractor(img2).view(img2.size(0), -1)\n\n        # ðŸ“Œ Concatenate features and pass through head\n        out = self.fc(torch.cat([f1, f2], dim=1))\n        return out","metadata":{"_uuid":"5f746bdd-c28b-42b4-99e0-15009c4a408a","_cell_guid":"9e1f769b-f716-4f6a-b88e-4aa5db5bbbd5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T03:09:40.883828Z","iopub.execute_input":"2025-07-01T03:09:40.884126Z","iopub.status.idle":"2025-07-01T03:09:40.890561Z","shell.execute_reply.started":"2025-07-01T03:09:40.884106Z","shell.execute_reply":"2025-07-01T03:09:40.889797Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport pandas as pd\n\n# ðŸ“Œ Function to train the Siamese Network\ndef train_siamese_model(\n    model, \n    train_dataset, \n    val_dataset, \n    epochs=10, \n    batch_size=32, \n    lr=1e-4,\n    save_dir='/kaggle/working/'\n):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n\n    # ðŸ“Œ Create data loaders for training and validation\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    criterion = nn.BCEWithLogitsLoss()  # ðŸ“Œ Binary cross-entropy loss with logits\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # ðŸ“Œ Adam optimizer\n\n    # ðŸ“Œ Track training metrics\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"val_loss\": [],\n        \"val_acc\": [],\n        \"precision\": [],\n        \"recall\": [],\n        \"f1\": []\n    }\n\n    best_val_acc = 0.0\n    best_model_path = None\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss, train_correct, total_train = 0.0, 0, 0\n        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n\n        # ðŸ“Œ Training loop\n        for batch in loop:\n            img1 = batch[\"img1\"].to(device)\n            img2 = batch[\"img2\"].to(device)\n            labels = batch[\"label\"].to(device)\n\n            attn = batch[\"attn\"].to(device) if USE_DCT_ATTENTION else None\n            outputs = model(img1, img2, attn).squeeze(1)\n\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            preds = (torch.sigmoid(outputs) > 0.5).float()\n            train_correct += (preds == labels).sum().item()\n            train_loss += loss.item() * labels.size(0)\n            total_train += labels.size(0)\n            loop.set_postfix(loss=loss.item())\n\n        train_acc = train_correct / total_train\n        train_loss = train_loss / total_train\n\n        # ðŸ“Œ Validation loop\n        model.eval()\n        val_loss, val_correct, total_val = 0.0, 0, 0\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            for batch in val_loader:\n                img1 = batch[\"img1\"].to(device)\n                img2 = batch[\"img2\"].to(device)\n                labels = batch[\"label\"].to(device)\n                attn = batch[\"attn\"].to(device) if USE_DCT_ATTENTION else None\n\n                outputs = model(img1, img2, attn).squeeze(1)\n                loss = criterion(outputs, labels)\n\n                probs = torch.sigmoid(outputs).cpu().numpy()\n                preds = (probs > 0.5).astype(float)\n                labels_np = labels.cpu().numpy()\n\n                all_preds.extend(preds)\n                all_labels.extend(labels_np)\n\n                val_correct += (preds == labels_np).sum().item()\n                val_loss += loss.item() * labels.size(0)\n                total_val += labels.size(0)\n\n        val_acc = val_correct / total_val\n        val_loss = val_loss / total_val\n        precision = precision_score(all_labels, all_preds, zero_division=0)\n        recall = recall_score(all_labels, all_preds, zero_division=0)\n        f1 = f1_score(all_labels, all_preds, zero_division=0)\n\n        print(f\"ðŸ“£ Epoch {epoch+1}: \"\n              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f} | \"\n              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n\n        # ðŸ“Œ Save best model based on validation accuracy\n        if val_acc > best_val_acc:\n            if best_model_path and os.path.exists(best_model_path):\n                os.remove(best_model_path)\n            model_name = f\"siamese_best_epoch{epoch+1}_acc{val_acc:.4f}.pt\"\n            best_model_path = os.path.join(save_dir, model_name)\n            torch.save(model.state_dict(), best_model_path)\n            best_val_acc = val_acc\n            print(f\"âœ… New best model saved: {model_name}\")\n\n        # ðŸ“Œ Update metrics history\n        history[\"epoch\"].append(epoch + 1)\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        history[\"precision\"].append(precision)\n        history[\"recall\"].append(recall)\n        history[\"f1\"].append(f1)\n\n    # ðŸ“Œ Save training metrics to CSV\n    history_df = pd.DataFrame(history)\n    history_df.to_csv(os.path.join(save_dir, \"training_metrics_cycle1.csv\"), index=False)\n    print(\"ðŸ“ˆ Training metrics saved to training_metrics.csv\")","metadata":{"_uuid":"16a53962-4e84-43dd-907b-e7f0143b3dd2","_cell_guid":"f9672b4b-5c4b-47b9-a0fb-a15c987dbf73","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T03:10:07.789805Z","iopub.execute_input":"2025-07-01T03:10:07.790069Z","iopub.status.idle":"2025-07-01T03:10:07.803645Z","shell.execute_reply.started":"2025-07-01T03:10:07.790028Z","shell.execute_reply":"2025-07-01T03:10:07.802875Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Initialize Siamese Network with ResNet18 backbone\nmodel_backbone_r18 = SiameseNet(backbone=\"resnet18\", pretrained=True)","metadata":{"_uuid":"ee293ca4-363c-4673-8c02-cc127b0f01bb","_cell_guid":"a5a2d5ea-11a6-4e4f-8098-5908484393d0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T03:18:24.288277Z","iopub.execute_input":"2025-07-01T03:18:24.288516Z","iopub.status.idle":"2025-07-01T03:18:24.493214Z","shell.execute_reply.started":"2025-07-01T03:18:24.288500Z","shell.execute_reply":"2025-07-01T03:18:24.492590Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Check GPU status\n!nvidia-smi","metadata":{"_uuid":"9b74c89e-23d4-439b-bd9f-1f68309f1142","_cell_guid":"4de1e29a-e19e-4ea7-806d-f7df01254dc2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-01T03:14:21.376817Z","iopub.execute_input":"2025-07-01T03:14:21.377120Z","iopub.status.idle":"2025-07-01T03:14:21.624772Z","shell.execute_reply.started":"2025-07-01T03:14:21.377098Z","shell.execute_reply":"2025-07-01T03:14:21.624060Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ Train the Siamese Network\ntrain_siamese_model(\n    model=model_backbone_r18,\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    epochs=5,                        # ðŸ“Œ Number of training epochs\n    batch_size=32,                   # ðŸ“Œ Batch size for training\n    lr=1e-4,                         # ðŸ“Œ Learning rate\n    save_dir=\"/kaggle/working\"\n)","metadata":{"_uuid":"1172f332-7897-4ef5-9366-b9939b52a12d","_cell_guid":"f3bb6509-fbfe-4570-8df2-632428161454","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}