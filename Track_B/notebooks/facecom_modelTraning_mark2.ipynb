{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12213659,"sourceType":"datasetVersion","datasetId":7694235},{"sourceId":455940,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":369720,"modelId":390610}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === Config ===\nUSE_DCT_ATTENTION = 0   # 🔄 flip to True later if you want attention\nIMG_SIZE = 224","metadata":{"_uuid":"8b9de698-772d-40b6-8f2f-c9a09122ec32","_cell_guid":"1da579c8-1ee6-4764-a3d1-92601884e01e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-02T08:14:52.969289Z","iopub.execute_input":"2025-07-02T08:14:52.970013Z","iopub.status.idle":"2025-07-02T08:14:52.973409Z","shell.execute_reply.started":"2025-07-02T08:14:52.969979Z","shell.execute_reply":"2025-07-02T08:14:52.972729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\n\n\"\"\"\nthe function below just loads picks up the right paths from the test or the validation directory and\narranges them in a dictionary\nThe directory structure is that say we pickup a random directory X... under it we will have one or multiple clean images \nand under sub-dir named distortion under which all the distorted images are present\n\"\"\"\n\ndef collect_image_paths(root_dir):\n    person_dict = {}\n\n    for person_folder in sorted(os.listdir(root_dir)):\n        folder_path = os.path.join(root_dir, person_folder)\n        if not os.path.isdir(folder_path):\n            continue\n\n        # Undistorted images (exclude anything in subfolders)\n        all_jpgs = glob.glob(os.path.join(folder_path, \"*.jpg\"))\n        clean_imgs = [f for f in all_jpgs if \"distortion\" not in f]\n\n        # Distorted images\n        distortion_dir = os.path.join(folder_path, \"distortion\")\n        distortion_imgs = []\n        if os.path.exists(distortion_dir):\n            distortion_imgs = glob.glob(os.path.join(distortion_dir, \"*.jpg\"))\n\n        if clean_imgs:\n            person_dict[person_folder] = {\n                \"clean\": clean_imgs,  # store list, not just one\n                \"distorted\": distortion_imgs\n            }\n\n    return person_dict","metadata":{"_uuid":"b4cdc445-e6bb-46a7-9728-86ad6ba351c6","_cell_guid":"80da059c-0703-4663-be84-c4b776bfec6c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:52.974959Z","iopub.execute_input":"2025-07-02T08:14:52.975178Z","iopub.status.idle":"2025-07-02T08:14:52.993346Z","shell.execute_reply.started":"2025-07-02T08:14:52.975163Z","shell.execute_reply":"2025-07-02T08:14:52.992830Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dir = f\"/kaggle/input/facecom/Comys_Hackathon5/Task_B/train\"\nperson_dict = collect_image_paths(train_dir)","metadata":{"_uuid":"5e7e7427-edbf-4f90-ab5c-8253df5a21ec","_cell_guid":"a62481bc-584e-4924-a8f8-fa6e9dbe8a5b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:52.994008Z","iopub.execute_input":"2025-07-02T08:14:52.994196Z","iopub.status.idle":"2025-07-02T08:14:55.164157Z","shell.execute_reply.started":"2025-07-02T08:14:52.994183Z","shell.execute_reply":"2025-07-02T08:14:55.163387Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_dir = f\"/kaggle/input/facecom/Comys_Hackathon5/Task_B/val\"\nval_dict = collect_image_paths(validation_dir)","metadata":{"_uuid":"04818321-9794-4021-9646-5ac388eceee7","_cell_guid":"07204e1e-d643-4d08-a4dd-c8bd0bb53c5a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:55.164971Z","iopub.execute_input":"2025-07-02T08:14:55.165184Z","iopub.status.idle":"2025-07-02T08:14:55.810264Z","shell.execute_reply.started":"2025-07-02T08:14:55.165163Z","shell.execute_reply":"2025-07-02T08:14:55.809717Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\ndef generate_balanced_augmented_pairs(person_dict, min_pos_per_id=28, num_neg_per_pos=3, seed=42):\n    \"\"\"\n    Generates positive and negative pairs for face verification task.\n    \n    - Ensures at least `min_pos_per_id` positive pairs per identity (if possible).\n    - For each positive, generates `num_neg_per_pos` negative pairs from other identities.\n    - Random sampling ensures balance and coverage of identities.\n\n    Args:\n        person_dict (dict): Dictionary with keys as person IDs and values as dicts with 'clean' and 'distorted' paths.\n        min_pos_per_id (int): Minimum number of positive pairs per identity.\n        num_neg_per_pos (int): Number of negative pairs to generate per positive pair.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        List of (img1_path, img2_path, label) tuples.\n    \"\"\"\n    random.seed(seed)\n    all_ids = list(person_dict.keys())\n    positive_pairs = []\n    negative_pairs = []\n\n    for person_id in all_ids:\n        images = []\n        # Gather all available images for positive pairing\n        if isinstance(person_dict[person_id]['clean'], list):\n            images += person_dict[person_id]['clean']\n        elif person_dict[person_id]['clean']:\n            images.append(person_dict[person_id]['clean'])\n\n        images += person_dict[person_id]['distorted']\n\n        # Ensure at least 2 images to form a pair\n        if len(images) < 2:\n            continue\n\n        # Generate all possible positive pairs (combinations of 2)\n        all_pos_pairs = [(a, b) for idx, a in enumerate(images) for b in images[idx+1:] if a != b]\n\n        # Sample up to min_pos_per_id (or fewer if not enough pairs exist)\n        selected_pos_pairs = random.sample(all_pos_pairs, min(min_pos_per_id, len(all_pos_pairs)))\n\n        for img1, img2 in selected_pos_pairs:\n            positive_pairs.append((img1, img2, 1))\n\n            # Generate negatives for each positive\n            other_ids = [pid for pid in all_ids if pid != person_id]\n            sampled_neg_ids = random.sample(other_ids, min(num_neg_per_pos, len(other_ids)))\n\n            for neg_id in sampled_neg_ids:\n                neg_candidates = person_dict[neg_id]['distorted'] or [person_dict[neg_id]['clean']]\n                if not neg_candidates:\n                    continue\n                neg_img = random.choice(neg_candidates)\n                # Use img1 from the positive pair to form the negative pair\n                negative_pairs.append((img1, neg_img, 0))\n\n    # Combine and shuffle\n    all_pairs = positive_pairs + negative_pairs\n    random.shuffle(all_pairs)\n    return all_pairs, positive_pairs, negative_pairs","metadata":{"_uuid":"23b2a02e-508a-4f6f-9c59-972b764099ec","_cell_guid":"bebdd707-ba98-4790-8685-bc55b81a0604","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:55.812101Z","iopub.execute_input":"2025-07-02T08:14:55.812361Z","iopub.status.idle":"2025-07-02T08:14:55.819574Z","shell.execute_reply.started":"2025-07-02T08:14:55.812344Z","shell.execute_reply":"2025-07-02T08:14:55.818905Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_pairs, positive_pairs, negative_pairs = generate_balanced_augmented_pairs(\n    person_dict=person_dict, \n    min_pos_per_id=28, \n    num_neg_per_pos=1\n)","metadata":{"_uuid":"f336b3d6-2055-437f-9f74-68a5f64994eb","_cell_guid":"2e171279-6ade-44c3-ad19-d2f8227a364b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:55.820206Z","iopub.execute_input":"2025-07-02T08:14:55.820416Z","iopub.status.idle":"2025-07-02T08:14:56.807472Z","shell.execute_reply.started":"2025-07-02T08:14:55.820397Z","shell.execute_reply":"2025-07-02T08:14:56.806943Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_val_pairs, val_pos , val_neg = generate_balanced_augmented_pairs(val_dict,20,1)","metadata":{"_uuid":"edc57578-ae47-4b31-89f2-dabb93b546cd","_cell_guid":"dd8da185-0414-43a8-b83e-4c3d45630b60","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:56.808137Z","iopub.execute_input":"2025-07-02T08:14:56.808385Z","iopub.status.idle":"2025-07-02T08:14:56.872414Z","shell.execute_reply.started":"2025-07-02T08:14:56.808360Z","shell.execute_reply":"2025-07-02T08:14:56.871901Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Stats\nprint(f\"✅ Total pairs: {len(all_pairs)}\")\nprint(f\"🔵 Positive pairs: {len(positive_pairs)}\")\nprint(f\"🔴 Negative pairs: {len(negative_pairs)}\")\nprint(\"🧾 Sample pairs:\", all_pairs[:3])\n\n# Step 4: Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\npair_stats = {\"identity\": [], \"label\": []}\nfor img1, img2, label in all_pairs:\n    identity = os.path.basename(os.path.dirname(img1))\n    pair_stats[\"identity\"].append(identity)\n    pair_stats[\"label\"].append(label)\n\ndf_pairs = pd.DataFrame(pair_stats)\n\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df_pairs, x=\"label\", palette=\"Set2\")\nplt.xticks([0, 1], [\"Negative\", \"Positive\"])\nplt.title(\"Distribution of Positive vs Negative Pairs\")\nplt.xlabel(\"Pair Type\")\nplt.ylabel(\"Count\")\nplt.grid(True, axis='y')\nplt.show()","metadata":{"_uuid":"f1430e72-e165-4f86-8347-41adee7cddd5","_cell_guid":"34ca2445-244f-4c06-8343-502e55c1288d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:56.873074Z","iopub.execute_input":"2025-07-02T08:14:56.873237Z","iopub.status.idle":"2025-07-02T08:14:57.069310Z","shell.execute_reply.started":"2025-07-02T08:14:56.873224Z","shell.execute_reply":"2025-07-02T08:14:57.068709Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Stats\nprint(f\"✅ Total pairs: {len(all_val_pairs)}\")\nprint(f\"🔵 Positive pairs: {len(val_pos)}\")\nprint(f\"🔴 Negative pairs: {len(val_neg)}\")\nprint(\"🧾 Sample pairs:\", all_pairs[:3])\n\n# Step 4: Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\npair_stats = {\"identity\": [], \"label\": []}\nfor img1, img2, label in all_val_pairs:\n    identity = os.path.basename(os.path.dirname(img1))\n    pair_stats[\"identity\"].append(identity)\n    pair_stats[\"label\"].append(label)\n\ndf_pairs = pd.DataFrame(pair_stats)\n\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df_pairs, x=\"label\", palette=\"Set2\")\nplt.xticks([0, 1], [\"Negative\", \"Positive\"])\nplt.title(\"Distribution of Positive vs Negative Pairs\")\nplt.xlabel(\"Pair Type\")\nplt.ylabel(\"Count\")\nplt.grid(True, axis='y')\nplt.show()","metadata":{"_uuid":"e44be53e-9449-4701-8b2f-151f5532b369","_cell_guid":"ff0984b0-205f-402b-822e-94520b8e1c80","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.070047Z","iopub.execute_input":"2025-07-02T08:14:57.070272Z","iopub.status.idle":"2025-07-02T08:14:57.211465Z","shell.execute_reply.started":"2025-07-02T08:14:57.070246Z","shell.execute_reply":"2025-07-02T08:14:57.210892Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as T\nfrom PIL import Image\n\nto_tensor = T.Compose([\n    T.Grayscale(),\n    T.Resize((224, 224)),\n    T.ToTensor()\n])\n\ndef compute_fft_attention_batch(batch1, batch2):\n    f1 = torch.fft.fft2(batch1)             # [B, H, W]\n    f2 = torch.fft.fft2(batch2)\n    diff = torch.abs(f1 - f2)\n    attn_maps = torch.fft.ifft2(diff).real  # [B, H, W]\n    attn_maps -= attn_maps.amin(dim=(1, 2), keepdim=True)\n    attn_maps /= (attn_maps.amax(dim=(1, 2), keepdim=True) + 1e-8)\n    attn_maps = 1.0 - attn_maps\n    return attn_maps.unsqueeze(1)  # [B, 1, H, W]","metadata":{"_uuid":"00fbdc52-c0aa-4500-adf7-5656ec266ae8","_cell_guid":"e30dfa2a-c1e8-4ad1-81d8-2dacd5a0520e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.212131Z","iopub.execute_input":"2025-07-02T08:14:57.212350Z","iopub.status.idle":"2025-07-02T08:14:57.217807Z","shell.execute_reply.started":"2025-07-02T08:14:57.212334Z","shell.execute_reply":"2025-07-02T08:14:57.217214Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, hashlib\nfrom torchvision.transforms import Compose, Grayscale, Resize, ToTensor\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nresize = Resize((224, 224))\nattention_root = \"/kaggle/working/fft_attention_maps\"\nos.makedirs(attention_root, exist_ok=True)\n\nto_tensor = Compose([Grayscale(), resize, ToTensor()])","metadata":{"_uuid":"a4c0ce5e-06ef-4b39-bcc2-5247bd5dd054","_cell_guid":"5e5cd389-80e8-478f-a073-49ba260fcd35","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.218549Z","iopub.execute_input":"2025-07-02T08:14:57.218820Z","iopub.status.idle":"2025-07-02T08:14:57.235624Z","shell.execute_reply.started":"2025-07-02T08:14:57.218798Z","shell.execute_reply":"2025-07-02T08:14:57.234947Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _cache_key(path1, path2):\n    a, b = sorted([os.path.abspath(path1), os.path.abspath(path2)])\n    key = hashlib.md5(f\"{a}|{b}\".encode()).hexdigest()\n    return os.path.join(attention_root, f\"{key}.pt\")","metadata":{"_uuid":"7c8ea755-cc72-4aa1-96d3-9fe6780c267d","_cell_guid":"76ff0ca1-3697-4c13-b0fd-211422ce3418","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.236373Z","iopub.execute_input":"2025-07-02T08:14:57.236649Z","iopub.status.idle":"2025-07-02T08:14:57.255722Z","shell.execute_reply.started":"2025-07-02T08:14:57.236628Z","shell.execute_reply":"2025-07-02T08:14:57.255116Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def batch_cache_attention(pairs, batch_size=32):\n    uncached = []\n    paths = []\n\n    for p1, p2, _ in pairs:\n        out_path = _cache_key(p1, p2)\n        if not os.path.exists(out_path):\n            uncached.append((p1, p2))\n            paths.append(out_path)\n\n    if not uncached:\n        print(\"✅ All attention maps already cached.\")\n        return\n\n    for i in tqdm(range(0, len(uncached), batch_size), desc=\"⚡ Batch-caching attention maps\"):\n        batch = uncached[i:i + batch_size]\n        batch_paths = paths[i:i + batch_size]\n\n        imgs1 = []\n        imgs2 = []\n\n        for p1, p2 in batch:\n            try:\n                imgs1.append(to_tensor(Image.open(p1).convert(\"RGB\")))\n                imgs2.append(to_tensor(Image.open(p2).convert(\"RGB\")))\n            except Exception as e:\n                print(f\"❌ Failed to load: {p1} or {p2} — {e}\")\n\n        if not imgs1 or not imgs2:\n            continue\n\n        t1 = torch.stack(imgs1).squeeze(1).to(device)  # [B, H, W]\n        t2 = torch.stack(imgs2).squeeze(1).to(device)\n\n        attn_batch = compute_fft_attention_batch(t1, t2).cpu()  # [B, 1, H, W]\n\n        for attn_map, out_path in zip(attn_batch, batch_paths):\n            torch.save(attn_map.half(), out_path, pickle_protocol=4)","metadata":{"_uuid":"dca9ee93-52f6-440c-8380-d79149bfec3b","_cell_guid":"6596539c-7786-4493-8571-90cd40783205","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.257920Z","iopub.execute_input":"2025-07-02T08:14:57.258143Z","iopub.status.idle":"2025-07-02T08:14:57.278922Z","shell.execute_reply.started":"2025-07-02T08:14:57.258128Z","shell.execute_reply":"2025-07-02T08:14:57.278295Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming: all_to_cache = positive_pairs + negative_pairs\nif USE_DCT_ATTENTION:\n    batch_cache_attention(all_pairs, batch_size=32)\nelse:\n    print(\"⚡ Skipping FFT-attention caching (USE_DCT_ATTENTION = False)\")","metadata":{"_uuid":"dfc223e9-fbe4-48ab-b787-eec67aa66f81","_cell_guid":"f3c91224-657d-47c8-8b8f-3f80f0b20178","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.279466Z","iopub.execute_input":"2025-07-02T08:14:57.279706Z","iopub.status.idle":"2025-07-02T08:14:57.296604Z","shell.execute_reply.started":"2025-07-02T08:14:57.279668Z","shell.execute_reply":"2025-07-02T08:14:57.295964Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming: all_to_cache = positive_pairs + negative_pairs\nif USE_DCT_ATTENTION:\n    batch_cache_attention(all_val_pairs, batch_size=32)\nelse:\n    print(\"⚡ Skipping FFT-attention caching (USE_DCT_ATTENTION = False)\")","metadata":{"_uuid":"9743e744-e7d2-4e50-b6f5-d619398756d5","_cell_guid":"f3007352-3977-48d6-bd61-f6bcb7734ccf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.297347Z","iopub.execute_input":"2025-07-02T08:14:57.297547Z","iopub.status.idle":"2025-07-02T08:14:57.309691Z","shell.execute_reply.started":"2025-07-02T08:14:57.297532Z","shell.execute_reply":"2025-07-02T08:14:57.309162Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport hashlib\n\ndef reverse_lookup_path(attn_path, all_pairs):\n    target_hash = os.path.basename(attn_path).replace(\".pt\", \"\")\n\n    for path1, path2, _ in all_pairs:\n        a, b = sorted([os.path.abspath(path1), os.path.abspath(path2)])\n        key_raw = f\"{a}|{b}\"\n        key_hash = hashlib.md5(key_raw.encode()).hexdigest()\n        if key_hash == target_hash:\n            return path1, path2\n    return None, None\n\nfrom PIL import Image\nimport numpy as np\nfrom scipy.fftpack import dct, idct\nimport torch\n\ndef apply_2d_dct(img):\n    return dct(dct(img.T, norm='ortho').T, norm='ortho')\n\ndef apply_2d_idct(coeffs):\n    return idct(idct(coeffs.T, norm='ortho').T, norm='ortho')\n\ndef compute_dct_attention(img1, img2):\n    img1_np = np.array(img1.convert(\"L\"), dtype=np.float32)\n    img2_np = np.array(img2.convert(\"L\"), dtype=np.float32)\n    dct1 = apply_2d_dct(img1_np)\n    dct2 = apply_2d_dct(img2_np)\n    diff = np.abs(dct1 - dct2)\n    attn_map = apply_2d_idct(diff)\n    attn_map -= attn_map.min()\n    attn_map /= (attn_map.max() + 1e-8)\n    attn_map = 1.0 - attn_map\n    return torch.tensor(attn_map).float()","metadata":{"_uuid":"89fc2e1b-cebd-40eb-97d1-8b24e6d20eb0","_cell_guid":"959ae51f-5139-4405-bb2a-30f5c19022b3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.310355Z","iopub.execute_input":"2025-07-02T08:14:57.310512Z","iopub.status.idle":"2025-07-02T08:14:57.348057Z","shell.execute_reply.started":"2025-07-02T08:14:57.310499Z","shell.execute_reply":"2025-07-02T08:14:57.347358Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Load cached attention map\nattn_cached = torch.load(attn_path, weights_only=False).squeeze()\n\n# Lookup original images\nimg1_path, img2_path = reverse_lookup_path(attn_path, all_pairs)\nassert img1_path and img2_path, \"❌ Original paths not found!\"\n\n# Recompute DCT attention map\nimg1 = Image.open(img1_path).convert(\"RGB\")\nimg2 = Image.open(img2_path).convert(\"RGB\")\nattn_dct = compute_dct_attention(img1, img2)\n\n# Resize to match if needed\nif attn_cached.shape != attn_dct.shape:\n    from torchvision.transforms import Resize\n    attn_dct = Resize(attn_cached.shape)(attn_dct.unsqueeze(0)).squeeze(0)\n\n# Show both\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(attn_cached.cpu().numpy(), cmap='viridis')\nplt.title(\"Cached Attention Map\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(attn_dct.cpu().numpy(), cmap='viridis')\nplt.title(\"Recomputed DCT Attention\")\nplt.axis(\"off\")\n\nplt.suptitle(\"Sanity Check: Cached vs DCT Recomputed\", fontsize=14)\nplt.show()","metadata":{"_uuid":"b89c8906-0612-47d2-b4ca-0e95b5870bcb","_cell_guid":"10d21acf-855f-4e62-a644-97c828e9ea05","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-02T08:14:57.348801Z","iopub.execute_input":"2025-07-02T08:14:57.349195Z","iopub.status.idle":"2025-07-02T08:14:57.367430Z","shell.execute_reply.started":"2025-07-02T08:14:57.349175Z","shell.execute_reply":"2025-07-02T08:14:57.366488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, hashlib\nfrom torchvision.transforms import Compose, Grayscale, Resize, ToTensor\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nresize = Resize((224, 224))\nattention_root = \"/kaggle/working/fft_attention_maps\"\nos.makedirs(attention_root, exist_ok=True)\n\nto_tensor = Compose([Grayscale(), resize, ToTensor()])","metadata":{"_uuid":"5bd5b5a7-cb70-4d94-a5b7-89b4f43613ae","_cell_guid":"66776070-fce4-412f-8390-09aa2666aea0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:15:50.487729Z","iopub.execute_input":"2025-07-02T08:15:50.488050Z","iopub.status.idle":"2025-07-02T08:15:50.492839Z","shell.execute_reply.started":"2025-07-02T08:15:50.488032Z","shell.execute_reply":"2025-07-02T08:15:50.492254Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nclass FacePairDataset(Dataset):\n    def __init__(self, pairs, transform=None):\n        self.pairs = pairs\n        self.transform = transform or T.Compose([\n            T.Resize((IMG_SIZE, IMG_SIZE)), T.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        p1, p2, label = self.pairs[idx]\n        img1 = Image.open(p1).convert(\"RGB\")\n        img2 = Image.open(p2).convert(\"RGB\")\n        img1_t = self.transform(img1)\n        img2_t = self.transform(img2)\n        label_t = torch.tensor(label, dtype=torch.float32)\n\n        if USE_DCT_ATTENTION:\n            attn = torch.load(_cache_key(p1, p2), weights_only=False)\n            return {\"img1\": img1_t, \"img2\": img2_t, \"attn\": attn, \"label\": label_t}\n        else:\n            return {\"img1\": img1_t, \"img2\": img2_t, \"label\": label_t}","metadata":{"_uuid":"f0e9e4fa-4429-46de-9eb2-ccb05cfe44ea","_cell_guid":"0c24b901-4326-49ed-9386-0326d7b5e7f5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:15:54.769813Z","iopub.execute_input":"2025-07-02T08:15:54.770085Z","iopub.status.idle":"2025-07-02T08:15:54.776057Z","shell.execute_reply.started":"2025-07-02T08:15:54.770063Z","shell.execute_reply":"2025-07-02T08:15:54.775377Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = FacePairDataset(all_pairs)\nprint(\"Total samples:\", len(train_dataset))\n\n# View one sample\nsample = train_dataset[0]\nimg1  = sample[\"img1\"]\nimg2  = sample[\"img2\"]\nattn  = sample.get(\"attn\", None)  # safe even if attention is off\nlabel = sample[\"label\"]\n\nprint(f\"Image 1 shape       : {img1.shape}\")\nprint(f\"Image 2 shape       : {img2.shape}\")\nif attn is not None:\n    print(f\"Attention map shape : {attn.shape}\")\nelse:\n    print(\"Attention map       : ❌ Not used (USE_DCT_ATTENTION = False)\")\nprint(f\"Label               : {label}\")","metadata":{"_uuid":"9a8eaed1-0059-460c-91fa-e29b78a1fe64","_cell_guid":"946fd4a5-f784-4d35-912b-19915775d517","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:16:01.131568Z","iopub.execute_input":"2025-07-02T08:16:01.132071Z","iopub.status.idle":"2025-07-02T08:16:01.178263Z","shell.execute_reply.started":"2025-07-02T08:16:01.132048Z","shell.execute_reply":"2025-07-02T08:16:01.177578Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_dataset = FacePairDataset(all_val_pairs)\nprint(\"Total samples:\", len(val_dataset))\n\n# View one sample\nsample = val_dataset[0]\nimg1  = sample[\"img1\"]\nimg2  = sample[\"img2\"]\nattn  = sample.get(\"attn\", None)  # safe even if attention is off\nlabel = sample[\"label\"]\n\nprint(f\"Image 1 shape       : {img1.shape}\")\nprint(f\"Image 2 shape       : {img2.shape}\")\nif attn is not None:\n    print(f\"Attention map shape : {attn.shape}\")\nelse:\n    print(\"Attention map       : ❌ Not used (USE_DCT_ATTENTION = False)\")\nprint(f\"Label               : {label}\")","metadata":{"_uuid":"44e4294b-1827-4319-b100-28db7c41d924","_cell_guid":"3a9be7be-1176-4765-b58f-b8da749203db","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:16:05.053449Z","iopub.execute_input":"2025-07-02T08:16:05.053742Z","iopub.status.idle":"2025-07-02T08:16:05.244958Z","shell.execute_reply.started":"2025-07-02T08:16:05.053720Z","shell.execute_reply":"2025-07-02T08:16:05.244226Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# global toggle (make sure this exists in a config cell)\n# USE_DCT_ATTENTION = False\n\nclass SiameseNet(nn.Module):          # renamed (name is up to you)\n    def __init__(self, backbone=\"resnet18\", pretrained=True):\n        super().__init__()\n\n        # backbone (easily switchable)\n        base = getattr(models, backbone)(pretrained=pretrained)\n        self.feature_extractor = nn.Sequential(*list(base.children())[:-1])  # -> [B, 512, 1, 1]\n\n        # head\n        self.fc = nn.Sequential(\n            nn.Linear(512 * 2, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 1)\n        )\n\n    def forward(self, img1, img2, attn_map=None):\n        \"\"\"\n        img1, img2: [B, 3, H, W]\n        attn_map   : [B, 1, H, W] or None\n        \"\"\"\n        if USE_DCT_ATTENTION and attn_map is not None:\n            attn_map = attn_map.expand(-1, 3, -1, -1)  # broadcast to RGB\n            img1, img2 = img1 * attn_map, img2 * attn_map\n\n        f1 = self.feature_extractor(img1).view(img1.size(0), -1)  # [B, 512]\n        f2 = self.feature_extractor(img2).view(img2.size(0), -1)  # [B, 512]\n\n        out = self.fc(torch.cat([f1, f2], dim=1))  # [B, 1]  (logits)\n        return out","metadata":{"_uuid":"b3bae095-fcaa-45ee-b86e-6b9af1300065","_cell_guid":"46a74ce5-980b-4a21-bd25-6058762cba12","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:16:07.995479Z","iopub.execute_input":"2025-07-02T08:16:07.996045Z","iopub.status.idle":"2025-07-02T08:16:08.002304Z","shell.execute_reply.started":"2025-07-02T08:16:07.996023Z","shell.execute_reply":"2025-07-02T08:16:08.001604Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler  # ✅ modern AMP usage\nfrom tqdm.auto import tqdm\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport pandas as pd\nimport torch\nimport os\n\ndef train_siamese_model_amp(\n    model,\n    train_dataset,\n    val_dataset,\n    epochs=20,\n    batch_size=32,\n    lr=1e-4,\n    save_dir='/kaggle/working/',\n    use_amp=True,\n    patience=5,\n    use_dct_attention=True\n):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n    criterion = nn.BCEWithLogitsLoss()\n    scaler = GradScaler() if use_amp else None\n\n    history = {\n        \"epoch\": [], \"train_loss\": [], \"train_acc\": [],\n        \"val_loss\": [], \"val_acc\": [],\n        \"precision\": [], \"recall\": [], \"f1\": []\n    }\n\n    best_val_acc = 0.0\n    bad_epochs = 0\n    best_model_path = None\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss, train_correct, total_train = 0.0, 0, 0\n\n        print(f\"\\n📚 Epoch {epoch+1}/{epochs}\")\n        loop = tqdm(train_loader, desc=\"🔁 Training\", leave=False)\n\n        for batch in loop:\n            img1 = batch[\"img1\"].to(device)\n            img2 = batch[\"img2\"].to(device)\n            labels = batch[\"label\"].to(device)\n            attn = batch[\"attn\"].to(device) if use_dct_attention and \"attn\" in batch else None\n\n            optimizer.zero_grad()\n\n            with autocast(device_type='cuda', enabled=use_amp):\n                outputs = model(img1, img2, attn).squeeze(1)\n                loss = criterion(outputs, labels)\n\n            if use_amp:\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                optimizer.step()\n\n            preds = (torch.sigmoid(outputs) > 0.5).float()\n            train_correct += (preds == labels).sum().item()\n            train_loss += loss.item() * labels.size(0)\n            total_train += labels.size(0)\n\n            loop.set_postfix(loss=loss.item(), acc=train_correct / total_train)\n\n        train_loss = train_loss / total_train\n        train_acc = train_correct / total_train\n        scheduler.step()\n\n        # --- Validation ---\n        model.eval()\n        val_loss, val_correct, total_val = 0.0, 0, 0\n        all_preds, all_labels = [], []\n\n        with torch.no_grad():\n            for batch in val_loader:\n                img1 = batch[\"img1\"].to(device)\n                img2 = batch[\"img2\"].to(device)\n                labels = batch[\"label\"].to(device)\n                attn = batch[\"attn\"].to(device) if use_dct_attention and \"attn\" in batch else None\n\n                outputs = model(img1, img2, attn).squeeze(1)\n                loss = criterion(outputs, labels)\n\n                probs = torch.sigmoid(outputs).cpu().numpy()\n                preds = (probs > 0.5).astype(float)\n                labels_np = labels.cpu().numpy()\n\n                all_preds.extend(preds)\n                all_labels.extend(labels_np)\n\n                val_correct += (preds == labels_np).sum()\n                val_loss += loss.item() * labels.size(0)\n                total_val += labels.size(0)\n\n        val_acc = val_correct / total_val\n        val_loss = val_loss / total_val\n        precision = precision_score(all_labels, all_preds, zero_division=0)\n        recall = recall_score(all_labels, all_preds, zero_division=0)\n        f1 = f1_score(all_labels, all_preds, zero_division=0)\n\n        print(f\"📣 Epoch {epoch+1}: \"\n              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f} | \"\n              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n\n        history[\"epoch\"].append(epoch + 1)\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        history[\"precision\"].append(precision)\n        history[\"recall\"].append(recall)\n        history[\"f1\"].append(f1)\n\n        if val_acc > best_val_acc:\n            if best_model_path and os.path.exists(best_model_path):\n                os.remove(best_model_path)\n            best_val_acc = val_acc\n            bad_epochs = 0\n            model_name = f\"siamese_best_epoch{epoch+1}_acc{val_acc:.4f}.pt\"\n            best_model_path = os.path.join(save_dir, model_name)\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"✅ New best model saved: {model_name}\")\n        else:\n            bad_epochs += 1\n            print(f\"⚠️ No improvement. Bad epochs: {bad_epochs}/{patience}\")\n            if bad_epochs >= patience:\n                print(\"🛑 Early stopping triggered.\")\n                break\n\n    df = pd.DataFrame(history)\n    df.to_csv(os.path.join(save_dir, \"training_metrics_cycle1.csv\"), index=False)\n    print(\"📊 Training history saved to training_metrics_cycle1.csv\")\n","metadata":{"_uuid":"173f8057-15f1-403d-989f-a3a47041468a","_cell_guid":"68a55df7-9b6a-4b69-9426-56523d417400","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:49:20.082633Z","iopub.execute_input":"2025-07-02T08:49:20.083331Z","iopub.status.idle":"2025-07-02T08:49:30.113715Z","shell.execute_reply.started":"2025-07-02T08:49:20.083311Z","shell.execute_reply":"2025-07-02T08:49:30.112995Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_siamese_model_amp(\n    model=model,\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    epochs=25,\n    batch_size=64,\n    save_dir='/kaggle/working/',\n    use_amp=True,\n    patience=5,\n    use_dct_attention=True  # or False if not using DCT\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T08:49:30.114838Z","iopub.execute_input":"2025-07-02T08:49:30.115107Z","iopub.status.idle":"2025-07-02T08:49:35.890766Z","shell.execute_reply.started":"2025-07-02T08:49:30.115086Z","shell.execute_reply":"2025-07-02T08:49:35.889564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the training metrics\ndf = pd.read_csv('/kaggle/working/training_metrics_cycle1.csv')\n\n# Plotting\nplt.figure(figsize=(16, 10))\n\n# Loss\nplt.subplot(2, 2, 1)\nplt.plot(df['epoch'], df['train_loss'], label='Train Loss', marker='o')\nplt.plot(df['epoch'], df['val_loss'], label='Val Loss', marker='o')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss per Epoch\")\nplt.legend()\nplt.grid(True)\n\n# Accuracy\nplt.subplot(2, 2, 2)\nplt.plot(df['epoch'], df['train_acc'], label='Train Accuracy', marker='o')\nplt.plot(df['epoch'], df['val_acc'], label='Val Accuracy', marker='o')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy per Epoch\")\nplt.legend()\nplt.grid(True)\n\n# Precision and Recall\nplt.subplot(2, 2, 3)\nplt.plot(df['epoch'], df['precision'], label='Precision', marker='o')\nplt.plot(df['epoch'], df['recall'], label='Recall', marker='o')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.title(\"Precision & Recall per Epoch\")\nplt.legend()\nplt.grid(True)\n\n# F1 Score\nplt.subplot(2, 2, 4)\nplt.plot(df['epoch'], df['f1'], label='F1 Score', color='green', marker='o')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"F1 Score\")\nplt.title(\"F1 Score per Epoch\")\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"66653295-70c2-42df-a71a-059a85388eb2","_cell_guid":"5f3b07fd-6dab-4c59-bde4-e52ad9366064","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.377110Z","iopub.status.idle":"2025-07-02T08:14:57.377338Z","shell.execute_reply.started":"2025-07-02T08:14:57.377223Z","shell.execute_reply":"2025-07-02T08:14:57.377235Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Testing","metadata":{"_uuid":"75d29e94-7bb8-49c1-8df7-50e048f15b1a","_cell_guid":"d153776d-1dc9-4fe4-a653-4352185d6391","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\nimport glob\nimport random\n\n# --- Constants ---\nIMG_SIZE = 224\nUSE_DCT_ATTENTION = True  # True because you're using FFT attention\nTEST_MODEL_PATH = \"/kaggle/input/siamese_facecom/pytorch/default/1/siamese_best_epoch4_acc0.8825.pt\"\nTEST_DIR = \"/kaggle/input/facecom/Comys_Hackathon5/Task_B/val\"\nBATCH_SIZE = 32\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Load model ---\ntest_model = SiameseNet(backbone=\"resnet18\", pretrained=False)\ntest_model.load_state_dict(torch.load(TEST_MODEL_PATH, map_location=device))\ntest_model.to(device).eval()\n\n# --- Preprocessing ---\nimage_transform = T.Compose([\n    T.Resize((IMG_SIZE, IMG_SIZE)),\n    T.ToTensor()\n])\n\nfft_tensor = T.Compose([\n    T.Grayscale(),\n    T.Resize((IMG_SIZE, IMG_SIZE)),\n    T.ToTensor()\n])\n\ndef compute_fft_attention_batch(batch1, batch2):\n    f1 = torch.fft.fft2(batch1)\n    f2 = torch.fft.fft2(batch2)\n    diff = torch.abs(f1 - f2)\n    attn_maps = torch.fft.ifft2(diff).real\n    attn_maps -= attn_maps.amin(dim=(1, 2), keepdim=True)\n    attn_maps /= (attn_maps.amax(dim=(1, 2), keepdim=True) + 1e-8)\n    attn_maps = 1.0 - attn_maps\n    return attn_maps.unsqueeze(1)  # [B, 1, H, W]\n\n# --- Load image from path ---\ndef load_image_tensor(path, for_fft=False):\n    img = Image.open(path).convert(\"RGB\")\n    return (fft_tensor if for_fft else image_transform)(img)","metadata":{"_uuid":"a552d2a3-b33d-4a3d-8514-5e44dc3dcb41","_cell_guid":"9159225a-c2d6-406b-88e1-0dcc13c73eb4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.378465Z","iopub.status.idle":"2025-07-02T08:14:57.378760Z","shell.execute_reply.started":"2025-07-02T08:14:57.378614Z","shell.execute_reply":"2025-07-02T08:14:57.378626Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef evaluate_few_shot_siamese(model, support_dict, batch_size=5):\n    support_classes = list(support_dict.keys())\n    tqdm.write(f\"🔢 Total support classes: {len(support_classes)}\")\n\n    # Step 1: Select 1 clean image per class\n    support_images = []\n    support_fft_imgs = []\n    for c in support_classes:\n        clean_list = support_dict[c]['clean']\n        chosen_path = random.choice(clean_list) if isinstance(clean_list, list) else clean_list\n        try:\n            img = load_image_tensor(chosen_path).to(device)         # [3, H, W]\n            fft_img = load_image_tensor(chosen_path, for_fft=True).to(device)  # [H, W] or [1, H, W]\n            support_images.append(img)\n            support_fft_imgs.append(fft_img.squeeze(0) if fft_img.dim() == 3 else fft_img)\n        except Exception as e:\n            tqdm.write(f\"❌ Failed to load support image for class {c}: {e}\")\n            raise\n\n    # Step 2: Collect query images and labels\n    query_paths = []\n    query_labels = []\n    for cls in support_classes:\n        for q in support_dict[cls]['distorted']:\n            query_paths.append(q)\n            query_labels.append(cls)\n\n    tqdm.write(f\"🔍 Total query images: {len(query_paths)}\")\n    predicted_labels = []\n\n    for start in tqdm(range(0, len(query_paths), batch_size), desc=\"🧪 Inference\"):\n        end = min(start + batch_size, len(query_paths))\n        batch_paths = query_paths[start:end]\n        batch_labels = query_labels[start:end]\n        tqdm.write(f\"🧾 Processing batch: {start}-{end}\")\n\n        try:\n            query_imgs = torch.stack([load_image_tensor(p) for p in batch_paths]).to(device)  # [B, 3, H, W]\n            query_fft_imgs = torch.stack([load_image_tensor(p, for_fft=True) for p in batch_paths]).to(device)  # [B, H, W] or [B, 1, H, W]\n        except Exception as e:\n            tqdm.write(f\"❌ Error loading query batch: {e}\")\n            raise\n\n        # Ensure query_fft_imgs is [B, H, W]\n        if query_fft_imgs.dim() == 4 and query_fft_imgs.shape[1] == 1:\n            query_fft_imgs = query_fft_imgs.squeeze(1)\n\n            B = query_imgs.shape[0]\n            S = len(support_classes)\n        \n            # Make sure query_fft_imgs is [B, H, W]\n            if query_fft_imgs.dim() == 4 and query_fft_imgs.shape[1] == 1:\n                query_fft_imgs = query_fft_imgs.squeeze(1)\n        \n            support_imgs = torch.stack(support_images).to(device)        # [S, 3, H, W]\n            support_ffts = torch.stack(support_fft_imgs).to(device)      # [S, H, W]\n        \n            scores = []\n        \n            with torch.no_grad():\n                for i in range(S):\n                    support_img = support_imgs[i].unsqueeze(0).repeat(B, 1, 1, 1)  # [B, 3, H, W]\n                    support_fft = support_ffts[i].unsqueeze(0).repeat(B, 1, 1)     # [B, H, W]\n                    attn = compute_fft_attention_batch(support_fft, query_fft_imgs)  # [B, 1, H, W]\n                    logit = model(support_img, query_imgs, attn).squeeze(1)         # [B]\n                    scores.append(torch.sigmoid(logit))  # [B]\n        \n                scores = torch.stack(scores, dim=1)  # [B, S]\n                preds = torch.argmax(scores, dim=1).cpu().numpy()\n                batch_pred_labels = [support_classes[p] for p in preds]\n                predicted_labels.extend(batch_pred_labels)\n\n\n    acc = accuracy_score(query_labels, predicted_labels)\n    f1 = f1_score(query_labels, predicted_labels, average='macro')\n    return acc, f1","metadata":{"_uuid":"f9f45c81-3740-4cab-976f-28e46effbe64","_cell_guid":"1ad75615-6884-44a5-a57f-d95f43c7305b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.380121Z","iopub.status.idle":"2025-07-02T08:14:57.380368Z","shell.execute_reply.started":"2025-07-02T08:14:57.380268Z","shell.execute_reply":"2025-07-02T08:14:57.380278Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Run Evaluation ---\nval_dict = collect_image_paths(TEST_DIR)\nacc, f1 = evaluate_few_shot_siamese(test_model, val_dict)\nprint(f\"✅ Few-shot Classification Accuracy: {acc:.4f}\")\nprint(f\"✅ Macro-Averaged F1 Score: {f1:.4f}\")","metadata":{"_uuid":"25d07faa-d8ff-4f21-a45a-10ffa8782e26","_cell_guid":"cad868ca-1573-42e4-9716-2faabbb81613","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T08:14:57.381523Z","iopub.status.idle":"2025-07-02T08:14:57.381837Z","shell.execute_reply.started":"2025-07-02T08:14:57.381651Z","shell.execute_reply":"2025-07-02T08:14:57.381663Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"_uuid":"017533d8-8383-4bc0-91d6-14a0cf3d5b53","_cell_guid":"e4f6392e-cae0-439c-b7ae-3d9f8001c7f9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}