{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12213659,"sourceType":"datasetVersion","datasetId":7694235},{"sourceId":456425,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":370122,"modelId":391010},{"sourceId":457304,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":370822,"modelId":391722}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading Images #","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\n\n\"\"\"\nthe function below just loads picks up the right paths from the test or the validation directory and\narranges them in a dictionary\nThe directory structure is that say we pickup a random directory X... under it we will have one or multiple clean images \nand under sub-dir named distortion under which all the distorted images are present\n\"\"\"\n\ndef collect_image_paths(root_dir):\n    person_dict = {}\n\n    for person_folder in sorted(os.listdir(root_dir)):\n        folder_path = os.path.join(root_dir, person_folder)\n        if not os.path.isdir(folder_path):\n            continue\n\n        # Undistorted images (exclude anything in subfolders)\n        all_jpgs = glob.glob(os.path.join(folder_path, \"*.jpg\"))\n        clean_imgs = [f for f in all_jpgs if \"distortion\" not in f]\n\n        # Distorted images\n        distortion_dir = os.path.join(folder_path, \"distortion\")\n        distortion_imgs = []\n        if os.path.exists(distortion_dir):\n            distortion_imgs = glob.glob(os.path.join(distortion_dir, \"*.jpg\"))\n\n        if clean_imgs:\n            person_dict[person_folder] = {\n                \"clean\": clean_imgs,  # store list, not just one\n                \"distorted\": distortion_imgs\n            }\n\n    return person_dict","metadata":{"_uuid":"e4b22ec9-32f2-4c50-8d92-d407afb3734e","_cell_guid":"25a9c81a-e78b-43f6-8d38-bb78980aba16","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:19.043260Z","iopub.execute_input":"2025-07-03T10:11:19.043967Z","iopub.status.idle":"2025-07-03T10:11:19.052003Z","shell.execute_reply.started":"2025-07-03T10:11:19.043939Z","shell.execute_reply":"2025-07-03T10:11:19.051461Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = 224","metadata":{"_uuid":"5a20927f-a75a-41f1-b3d6-27105b789d49","_cell_guid":"ef32e493-b17e-4a90-8ad2-9a88eecc628c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:19.052655Z","iopub.execute_input":"2025-07-03T10:11:19.052980Z","iopub.status.idle":"2025-07-03T10:11:19.081264Z","shell.execute_reply.started":"2025-07-03T10:11:19.052948Z","shell.execute_reply":"2025-07-03T10:11:19.080784Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Load Image Paths**","metadata":{}},{"cell_type":"code","source":"testing_dir = f\"/kaggle/input/facecom/Comys_Hackathon5/Task_B/val\"\ntesting_dict = collect_image_paths(testing_dir)","metadata":{"_uuid":"118a03d5-5dc6-43cb-b2f3-a74397eed504","_cell_guid":"fa280666-2e00-47ed-acf3-8c5b3b208816","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:19.083212Z","iopub.execute_input":"2025-07-03T10:11:19.083420Z","iopub.status.idle":"2025-07-03T10:11:23.229625Z","shell.execute_reply.started":"2025-07-03T10:11:19.083404Z","shell.execute_reply":"2025-07-03T10:11:23.228805Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generating Pairs of Images from val directory for comparing similarity ","metadata":{}},{"cell_type":"code","source":"import random\n\ndef prepare_gallery_and_probe(person_dict, seed=42):\n    \"\"\"\n    From the person_dict:\n    - Pick 1 random clean image per identity for the gallery\n    - All remaining images (clean or distorted) are probes\n    \"\"\"\n    random.seed(seed)\n    gallery = {}  # label: image_path\n    probe_set = []  # (image_path, true_label)\n\n    label_map = {}  # person_folder → label (0 to 249)\n    for idx, (person_id, images) in enumerate(person_dict.items()):\n        # Assign unique label to this identity\n        label_map[person_id] = idx\n\n        clean_imgs = images[\"clean\"]\n        distorted_imgs = images[\"distorted\"]\n\n        # Randomly pick one clean image for gallery\n        gallery_img = random.choice(clean_imgs)\n        gallery[idx] = gallery_img  # label: path\n\n        # Add remaining clean images (excluding gallery_img) to probe\n        for img in clean_imgs:\n            if img != gallery_img:\n                probe_set.append((img, idx))\n\n        # Add all distorted images to probe\n        for img in distorted_imgs:\n            probe_set.append((img, idx))\n\n    return gallery, probe_set, label_map","metadata":{"_uuid":"e3806827-a4c8-4959-86f7-f4cb53b97d2c","_cell_guid":"02920a1a-ba2e-4e0d-ad07-074315cb30f2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:23.230283Z","iopub.execute_input":"2025-07-03T10:11:23.230551Z","iopub.status.idle":"2025-07-03T10:11:23.236278Z","shell.execute_reply.started":"2025-07-03T10:11:23.230524Z","shell.execute_reply":"2025-07-03T10:11:23.235499Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gallery, probe_set, label_map = prepare_gallery_and_probe(testing_dict)","metadata":{"_uuid":"53a4c256-0d1d-48c2-8657-c916a1ab26e2","_cell_guid":"33b0542a-9f10-4c61-95c7-450b123d9193","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:23.237074Z","iopub.execute_input":"2025-07-03T10:11:23.237267Z","iopub.status.idle":"2025-07-03T10:11:23.275161Z","shell.execute_reply.started":"2025-07-03T10:11:23.237251Z","shell.execute_reply":"2025-07-03T10:11:23.274576Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Total identities (gallery): {len(gallery)}\")\nprint(f\"Total probe images: {len(probe_set)}\")\nprint(f\"Sample probe image: {probe_set[0]}\")","metadata":{"_uuid":"6ade25bb-7f80-4a38-9945-91a562523c44","_cell_guid":"a8a05f9b-14ed-42c6-8d49-895b00b396e4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:23.275810Z","iopub.execute_input":"2025-07-03T10:11:23.276148Z","iopub.status.idle":"2025-07-03T10:11:23.294746Z","shell.execute_reply.started":"2025-07-03T10:11:23.276083Z","shell.execute_reply":"2025-07-03T10:11:23.294051Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport torchvision.transforms as T\n\ntransform = T.Compose([\n    T.Resize((IMG_SIZE, IMG_SIZE)),\n    T.ToTensor(),\n])\n\ndef load_image(path):\n    img = Image.open(path).convert(\"RGB\")\n    return transform(img)","metadata":{"_uuid":"30f65a41-60c3-4949-93ae-898bea1c8b67","_cell_guid":"4e75243c-6400-4104-8689-18541037fe83","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:23.295265Z","iopub.execute_input":"2025-07-03T10:11:23.295435Z","iopub.status.idle":"2025-07-03T10:11:35.239787Z","shell.execute_reply.started":"2025-07-03T10:11:23.295421Z","shell.execute_reply":"2025-07-03T10:11:35.239196Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Config ===\nUSE_DCT_ATTENTION = False   # 🔄 flip to True later if you want attention\nIMG_SIZE = 224","metadata":{"_uuid":"2f4ec756-4b00-46e4-b491-eeb7f6266200","_cell_guid":"ad394019-8192-494c-b9ce-dbbc51c7a3a8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:35.240530Z","iopub.execute_input":"2025-07-03T10:11:35.240850Z","iopub.status.idle":"2025-07-03T10:11:35.244651Z","shell.execute_reply.started":"2025-07-03T10:11:35.240829Z","shell.execute_reply":"2025-07-03T10:11:35.243981Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tqdm.notebook as tqdm","metadata":{"_uuid":"7e9798da-d734-4ff6-bc82-805563848119","_cell_guid":"944843f3-0ccd-4143-86f9-145c20ae6943","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:35.247122Z","iopub.execute_input":"2025-07-03T10:11:35.247320Z","iopub.status.idle":"2025-07-03T10:11:35.407623Z","shell.execute_reply.started":"2025-07-03T10:11:35.247304Z","shell.execute_reply":"2025-07-03T10:11:35.407118Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# global toggle (make sure this exists in a config cell)\n# USE_DCT_ATTENTION = False\n\nclass SiameseNet(nn.Module):          # renamed (name is up to you)\n    def __init__(self, backbone=\"resnet18\", pretrained=True):\n        super().__init__()\n\n        # backbone (easily switchable)\n        base = getattr(models, backbone)(pretrained=pretrained)\n        self.feature_extractor = nn.Sequential(*list(base.children())[:-1])  # -> [B, 512, 1, 1]\n\n        # head\n        self.fc = nn.Sequential(\n            nn.Linear(512 * 2, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 1)\n        )\n    \n    def extract_embedding(self, img):\n        with torch.no_grad():\n            f = self.feature_extractor(img).view(img.size(0), -1)  # [B, 512]\n        return f\n\n\n    def forward(self, img1, img2, attn_map=None):\n        \"\"\"\n        img1, img2: [B, 3, H, W]\n        attn_map   : [B, 1, H, W] or None\n        \"\"\"\n        if USE_DCT_ATTENTION and attn_map is not None:\n            attn_map = attn_map.expand(-1, 3, -1, -1)  # broadcast to RGB\n            img1, img2 = img1 * attn_map, img2 * attn_map\n\n        f1 = self.feature_extractor(img1).view(img1.size(0), -1)  # [B, 512]\n        f2 = self.feature_extractor(img2).view(img2.size(0), -1)  # [B, 512]\n\n        out = self.fc(torch.cat([f1, f2], dim=1))  # [B, 1]  (logits)\n        return out","metadata":{"_uuid":"4d6aba40-da74-4889-9e75-21cfdba3de70","_cell_guid":"650a9af1-7084-4c9a-92aa-a2a2daf79b85","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:35.408315Z","iopub.execute_input":"2025-07-03T10:11:35.408525Z","iopub.status.idle":"2025-07-03T10:11:35.415595Z","shell.execute_reply.started":"2025-07-03T10:11:35.408491Z","shell.execute_reply":"2025-07-03T10:11:35.414877Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_gallery_embeddings(model, gallery, device='cuda'):\n    model.eval()\n    embeddings = []\n    labels = []\n\n    with torch.no_grad():\n        for label, path in gallery.items():\n            img = load_image(path).unsqueeze(0).to(device)\n            emb = model.extract_embedding(img)  # [1, 512]\n            embeddings.append(emb)\n            labels.append(label)\n\n    gallery_tensor = torch.cat(embeddings, dim=0)  # [G, 512]\n    label_tensor = torch.tensor(labels)\n    return gallery_tensor, label_tensor","metadata":{"_uuid":"7f760008-bbcc-49c7-a808-2b312a88e932","_cell_guid":"bd6929f3-a39b-430a-b9c7-f585402b134c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:11:35.416317Z","iopub.execute_input":"2025-07-03T10:11:35.416563Z","iopub.status.idle":"2025-07-03T10:11:35.435256Z","shell.execute_reply.started":"2025-07-03T10:11:35.416543Z","shell.execute_reply":"2025-07-03T10:11:35.434546Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SiameseNet(backbone=\"resnet18\", pretrained=False)  # pretrained=False during load\nmodel.load_state_dict(torch.load(\"/kaggle/input/siamese1/pytorch/default/1/siamese_best_epoch2_acc0.8858.pt\", map_location='cuda'))  # or 'cuda'\nmodel.to('cuda')\nmodel.eval()","metadata":{"_uuid":"21485c43-3734-4501-8138-0ac465ca071a","_cell_guid":"5791529e-f8a2-4e09-8bd3-6eb33dad87b5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:12:02.879626Z","iopub.execute_input":"2025-07-03T10:12:02.880364Z","iopub.status.idle":"2025-07-03T10:12:04.096248Z","shell.execute_reply.started":"2025-07-03T10:12:02.880340Z","shell.execute_reply":"2025-07-03T10:12:04.095639Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing Script ","metadata":{}},{"cell_type":"code","source":"gallery_embs, gallery_labels = build_gallery_embeddings(model, gallery, device='cuda')\n\n# Normalize for cosine similarity (optional but recommended)\ngallery_embs = torch.nn.functional.normalize(gallery_embs, p=2, dim=1)\ngallery_labels = gallery_labels.to('cuda')","metadata":{"_uuid":"9b1feabb-8d53-450d-8a09-8feeee091328","_cell_guid":"055e3cef-4f6b-4e06-9fcf-36a13056ad8a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:12:08.170034Z","iopub.execute_input":"2025-07-03T10:12:08.170642Z","iopub.status.idle":"2025-07-03T10:12:15.184405Z","shell.execute_reply.started":"2025-07-03T10:12:08.170620Z","shell.execute_reply":"2025-07-03T10:12:15.183498Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluation based on class prediction by cosine similarity**","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass ProbeDataset(Dataset):\n    def __init__(self, probe_set):\n        self.data = probe_set\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        path, label = self.data[idx]\n        img = load_image(path)\n        return img, label\n\ndef evaluate_batch(model, gallery_embs, gallery_labels, probe_set, batch_size=32, device='cuda'):\n    model.eval()\n    probe_loader = DataLoader(ProbeDataset(probe_set), batch_size=batch_size, shuffle=False)\n\n    y_true = []\n    y_pred = []\n\n    with torch.no_grad():\n        for imgs, labels in tqdm.tqdm(probe_loader, desc=\"Evaluating\"):\n            imgs = imgs.to(device)  # [B, 3, H, W]\n            labels = labels.tolist()\n\n            probe_embs = model.extract_embedding(imgs)  # [B, 512]\n\n            # Cosine similarity: higher is more similar\n            sims = torch.matmul(probe_embs, gallery_embs.T)  # [B, G]\n            preds = torch.argmax(sims, dim=1)  # [B]\n            pred_labels = gallery_labels[preds].tolist()\n\n            y_true.extend(labels)\n            y_pred.extend(pred_labels)\n\n    return y_true, y_pred","metadata":{"_uuid":"1ab1588e-feda-4848-8872-aa07c1b260de","_cell_guid":"1a56db5f-45de-4b1b-85c4-aab91bc1be0a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:12:16.907743Z","iopub.execute_input":"2025-07-03T10:12:16.908037Z","iopub.status.idle":"2025-07-03T10:12:16.914874Z","shell.execute_reply.started":"2025-07-03T10:12:16.908016Z","shell.execute_reply":"2025-07-03T10:12:16.914105Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true, y_pred = evaluate_batch(model, gallery_embs, gallery_labels, probe_set, batch_size=32, device='cuda')","metadata":{"_uuid":"f6415102-4116-4b97-b414-24fc91fe0900","_cell_guid":"67591d4d-4620-4849-a5b2-c8f363971460","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:12:19.519926Z","iopub.execute_input":"2025-07-03T10:12:19.520226Z","iopub.status.idle":"2025-07-03T10:13:10.366944Z","shell.execute_reply.started":"2025-07-03T10:12:19.520205Z","shell.execute_reply":"2025-07-03T10:13:10.366127Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Some sample predictions**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n\ndef show_random_predictions(probe_set, y_true, y_pred,gallery, num_samples=20):\n    indices = random.sample(range(len(probe_set)), num_samples)\n\n    plt.figure(figsize=(15, num_samples * 2.5))  # Wider for 3 columns\n    for i, idx in enumerate(indices):\n        img_path, true_label = probe_set[idx]\n        pred_label = y_pred[idx]\n        \n        # Load images\n        img_probe = Image.open(img_path).convert(\"RGB\")\n        img_gallery_true = Image.open(gallery[true_label]).convert(\"RGB\")\n        img_gallery_pred = Image.open(gallery[pred_label]).convert(\"RGB\")\n        \n        # Column 1: Probe image\n        plt.subplot(num_samples, 3, 3 * i + 1)\n        plt.imshow(img_probe)\n        plt.axis('off')\n        plt.title(f\"Probe\\nT:{true_label} / P:{pred_label}\", color='green' if true_label == pred_label else 'red')\n        \n        # Column 2: True gallery\n        plt.subplot(num_samples, 3, 3 * i + 2)\n        plt.imshow(img_gallery_true)\n        plt.axis('off')\n        plt.title(f\"True Gallery\\n{true_label}\", color='blue')\n        \n        # Column 3: Predicted gallery\n        plt.subplot(num_samples, 3, 3 * i + 3)\n        plt.imshow(img_gallery_pred)\n        plt.axis('off')\n        plt.title(f\"Predicted Gallery\\n{pred_label}\", color='orange')\n\n\n    plt.tight_layout()\n    plt.show()\n\n# 🔍 Run it\nshow_random_predictions(probe_set, y_true, y_pred, gallery, num_samples=20)","metadata":{"_uuid":"43979f1f-4fea-4d6d-aa71-ee3c058ff14a","_cell_guid":"103fb52c-4e34-4924-b18f-222d17016bed","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:13:23.921293Z","iopub.execute_input":"2025-07-03T10:13:23.921886Z","iopub.status.idle":"2025-07-03T10:13:30.658841Z","shell.execute_reply.started":"2025-07-03T10:13:23.921862Z","shell.execute_reply":"2025-07-03T10:13:30.657684Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation Results","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='macro')\n    return acc, f1","metadata":{"_uuid":"51664886-bac0-41b5-a10e-722c8e90ac28","_cell_guid":"41b054bf-90b1-427f-a4d4-30f6ad248ffa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-03T10:13:43.693898Z","iopub.execute_input":"2025-07-03T10:13:43.694186Z","iopub.status.idle":"2025-07-03T10:13:44.328022Z","shell.execute_reply.started":"2025-07-03T10:13:43.694166Z","shell.execute_reply":"2025-07-03T10:13:44.327444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc, f1 = compute_metrics(y_true, y_pred)\nprint(f\"Top-1 Accuracy: {acc:.4f}\")\nprint(f\"Macro F1 Score: {f1:.4f}\")","metadata":{"_uuid":"d2301dea-dcaf-473d-9874-3a71c51efeb9","_cell_guid":"c3fe03dc-49f5-4926-abe1-642513b00c83","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-03T10:13:49.898441Z","iopub.execute_input":"2025-07-03T10:13:49.898873Z","iopub.status.idle":"2025-07-03T10:13:49.912257Z","shell.execute_reply.started":"2025-07-03T10:13:49.898852Z","shell.execute_reply":"2025-07-03T10:13:49.911665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Get all probe embeddings\nprobe_loader = DataLoader(ProbeDataset(probe_set), batch_size=64, shuffle=False)\nall_embs, all_labels = [], []\n\nmodel.eval()\nwith torch.no_grad():\n    for imgs, labels in tqdm.tqdm(probe_loader, desc=\"Embed\"):\n        imgs = imgs.cuda()\n        embs = model.extract_embedding(imgs)\n        all_embs.append(embs.cpu())\n        all_labels.extend(labels)\n\nall_embs = torch.cat(all_embs, dim=0).numpy()\n\n# Reduce dimensions\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\nreduced_embs = tsne.fit_transform(all_embs)\n\n# Plot\nplt.figure(figsize=(10, 10))\nscatter = plt.scatter(reduced_embs[:, 0], reduced_embs[:, 1], c=all_labels, cmap='tab20', s=10)\nplt.title(\"t-SNE of Probe Embeddings\")\nplt.colorbar(scatter)\nplt.show()","metadata":{"_uuid":"2884eba7-55ee-429b-a658-a939a3a2ab8b","_cell_guid":"42d194b1-fa5a-436f-902e-34d1d041ff21","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:13:54.885025Z","iopub.execute_input":"2025-07-03T10:13:54.885316Z","iopub.status.idle":"2025-07-03T10:14:36.824730Z","shell.execute_reply.started":"2025-07-03T10:13:54.885295Z","shell.execute_reply":"2025-07-03T10:14:36.823954Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport numpy as np\n\n# Compute cosine similarity between first 50 embeddings\nfrom sklearn.metrics.pairwise import cosine_similarity\nsim_matrix = cosine_similarity(all_embs[:50])\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(sim_matrix, xticklabels=5, yticklabels=5, cmap=\"viridis\")\nplt.title(\"Cosine Similarity Between Probe Embeddings (Top 50)\")\nplt.show()","metadata":{"_uuid":"8685d4cb-e603-4b6d-9b0e-1598da45ab65","_cell_guid":"0899be97-f067-4b86-ae78-07144a44c4b5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:14:44.991897Z","iopub.execute_input":"2025-07-03T10:14:44.992653Z","iopub.status.idle":"2025-07-03T10:14:45.959696Z","shell.execute_reply.started":"2025-07-03T10:14:44.992626Z","shell.execute_reply":"2025-07-03T10:14:45.959079Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm, cmap=\"Blues\", xticklabels=False, yticklabels=False)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"_uuid":"36aecf10-df08-4cfb-9b07-d57d520f8c19","_cell_guid":"10597880-5bee-47a2-8721-2c8cacb3b733","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:14:50.759347Z","iopub.execute_input":"2025-07-03T10:14:50.760110Z","iopub.status.idle":"2025-07-03T10:14:50.964679Z","shell.execute_reply.started":"2025-07-03T10:14:50.760087Z","shell.execute_reply":"2025-07-03T10:14:50.963960Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_balanced_pairs(person_dict, max_pos_per_person=None, neg_per_pos=1):\n    pairs, labels = [], []\n    people = list(person_dict.keys())\n\n    for person in people:\n        imgs = person_dict[person][\"clean\"] + person_dict[person][\"distorted\"]\n        pos_pairs = []\n\n        # Positive pairs\n        for i in range(len(imgs)):\n            for j in range(i + 1, len(imgs)):\n                pos_pairs.append((imgs[i], imgs[j]))\n\n        if max_pos_per_person:\n            pos_pairs = random.sample(pos_pairs, min(len(pos_pairs), max_pos_per_person))\n\n        for pair in pos_pairs:\n            pairs.append(pair)\n            labels.append(1)\n\n            # Add negatives\n            neg_persons = [p for p in people if p != person]\n            for _ in range(neg_per_pos):\n                neg_p = random.choice(neg_persons)\n                neg_imgs = person_dict[neg_p][\"clean\"] + person_dict[neg_p][\"distorted\"]\n                if not neg_imgs:\n                    continue\n                neg_pair = (pair[0], random.choice(neg_imgs))\n                pairs.append(neg_pair)\n                labels.append(0)\n\n    return pairs, labels","metadata":{"_uuid":"cf69995b-d4cf-40d7-b432-7ce06e1cc43c","_cell_guid":"301475ee-db4f-4934-9be8-0e5cc5ea0a9c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:16:36.275861Z","iopub.execute_input":"2025-07-03T10:16:36.276255Z","iopub.status.idle":"2025-07-03T10:16:36.283151Z","shell.execute_reply.started":"2025-07-03T10:16:36.276230Z","shell.execute_reply":"2025-07-03T10:16:36.282561Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pairs, pair_labels = generate_balanced_pairs(\n    testing_dict,\n    max_pos_per_person=None,   # or None to use all positives\n    neg_per_pos=1            # e.g. 1 negative per positive\n)\n\nfrom collections import Counter\nprint(Counter(pair_labels))","metadata":{"_uuid":"98921141-4ace-4883-b25b-e4410850a51e","_cell_guid":"0662b345-a569-4fdb-b760-863afeef0402","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:17:09.899607Z","iopub.execute_input":"2025-07-03T10:17:09.900273Z","iopub.status.idle":"2025-07-03T10:17:10.354454Z","shell.execute_reply.started":"2025-07-03T10:17:09.900244Z","shell.execute_reply":"2025-07-03T10:17:10.353846Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from itertools import combinations, product\n\ndef generate_all_pairs(person_dict):\n    pairs = []\n    labels = []\n\n    people = list(person_dict.keys())\n\n    # Positive pairs\n    for person in people:\n        imgs = person_dict[person][\"clean\"] + person_dict[person][\"distorted\"]\n        for i, j in combinations(imgs, 2):\n            pairs.append((i, j))\n            labels.append(1)\n\n    # Negative pairs\n    for i in range(len(people)):\n        for j in range(i + 1, len(people)):\n            imgs1 = person_dict[people[i]][\"clean\"] + person_dict[people[i]][\"distorted\"]\n            imgs2 = person_dict[people[j]][\"clean\"] + person_dict[people[j]][\"distorted\"]\n            for img1, img2 in product(imgs1, imgs2):\n                pairs.append((img1, img2))\n                labels.append(0)\n\n    return pairs, labels","metadata":{"_uuid":"dd414302-0dd6-4e26-affb-a7aa511f6188","_cell_guid":"c79c1b8f-f251-4bc1-abcd-41a7f442fd31","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:17:17.771026Z","iopub.execute_input":"2025-07-03T10:17:17.771759Z","iopub.status.idle":"2025-07-03T10:17:17.777277Z","shell.execute_reply.started":"2025-07-03T10:17:17.771736Z","shell.execute_reply":"2025-07-03T10:17:17.776630Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SiamesePairDataset(Dataset):\n    def __init__(self, pairs, labels, transform):\n        self.pairs = pairs\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        img1 = self.transform(Image.open(self.pairs[idx][0]).convert(\"RGB\"))\n        img2 = self.transform(Image.open(self.pairs[idx][1]).convert(\"RGB\"))\n        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return img1, img2, label","metadata":{"_uuid":"1f90835c-b04f-4bd9-bcfd-24b2050ff20d","_cell_guid":"787eae4a-0776-40af-9a79-a9da43ca38ac","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:17:22.782568Z","iopub.execute_input":"2025-07-03T10:17:22.783125Z","iopub.status.idle":"2025-07-03T10:17:22.788340Z","shell.execute_reply.started":"2025-07-03T10:17:22.783104Z","shell.execute_reply":"2025-07-03T10:17:22.787415Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluation based on image-pair similarity**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n\ndef evaluate_siamese_binary(model, dataloader, device='cuda'):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    all_scores = []\n\n    with torch.no_grad():\n        for img1, img2, label in tqdm.tqdm(dataloader, desc=\"Evaluating pairs\"):\n            img1, img2 = img1.to(device), img2.to(device)\n            label = label.to(device)\n            \n\n            output = model(img1, img2)  # logits\n            prob = torch.sigmoid(output.view(-1))\n\n            pred = (prob > 0.5).float()\n\n            all_labels.extend(label.cpu().numpy())\n            all_preds.extend(pred.cpu().numpy())\n            all_scores.extend(prob.cpu().numpy())\n\n    # Metrics\n    acc = accuracy_score(all_labels, all_preds)\n    prec = precision_score(all_labels, all_preds)\n    rec = recall_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds)\n    try:\n        auc = roc_auc_score(all_labels, all_scores)\n    except:\n        auc = float('nan')\n\n    cm = confusion_matrix(all_labels, all_preds)\n\n    return acc, prec, rec, f1, auc, cm, all_labels, all_preds, all_scores","metadata":{"_uuid":"95456860-c65f-49c4-90af-92779fcd51dc","_cell_guid":"974f7f41-bd78-480f-abad-aa1ee9e0fe62","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:17:25.787446Z","iopub.execute_input":"2025-07-03T10:17:25.788181Z","iopub.status.idle":"2025-07-03T10:17:25.794356Z","shell.execute_reply.started":"2025-07-03T10:17:25.788156Z","shell.execute_reply":"2025-07-03T10:17:25.793685Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pair_dataset = SiamesePairDataset(pairs, pair_labels, transform=transform)\npair_loader = DataLoader(pair_dataset, batch_size=64, shuffle=False, num_workers=4)\n\nacc, prec, rec, f1, auc, cm, all_labels, all_preds, all_scores = evaluate_siamese_binary(model, pair_loader)\n\nprint(f\"Top-1 Accuracy: {acc:.4f}\")\nprint(f\"Precision     : {prec:.4f}\")\nprint(f\"Recall        : {rec:.4f}\")\nprint(f\"F1 Score      : {f1:.4f}\")\nprint(f\"ROC AUC       : {auc:.4f}\")","metadata":{"_uuid":"a5f0988f-edec-4294-bab4-317610de1404","_cell_guid":"0bbbefc6-81f4-4f55-a2ed-dce3551acf3d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:17:29.105657Z","iopub.execute_input":"2025-07-03T10:17:29.106115Z","iopub.status.idle":"2025-07-03T10:26:46.195427Z","shell.execute_reply.started":"2025-07-03T10:17:29.106090Z","shell.execute_reply":"2025-07-03T10:26:46.194489Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Not Same\", \"Same\"], yticklabels=[\"Not Same\", \"Same\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Siamese Pair Confusion Matrix\")\nplt.show()","metadata":{"_uuid":"4abe6d20-c808-44c4-aa23-2f923bbe207b","_cell_guid":"f80b72d2-ab4f-4f6b-9c21-7e76f85daa95","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-03T10:26:46.197095Z","iopub.execute_input":"2025-07-03T10:26:46.197342Z","iopub.status.idle":"2025-07-03T10:26:46.367214Z","shell.execute_reply.started":"2025-07-03T10:26:46.197318Z","shell.execute_reply":"2025-07-03T10:26:46.366548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\n# These come from evaluate_siamese_binary\nprecision, recall, thresholds = precision_recall_curve(all_labels, all_scores)\nf1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\nbest_idx = f1_scores.argmax()\nbest_thresh = thresholds[best_idx]\n\nprint(f\"🔍 Optimal threshold: {best_thresh:.4f}, F1 at best threshold: {f1_scores[best_idx]:.4f}\")","metadata":{"_uuid":"50147f05-5ce5-4f3d-85b7-06340fafe43a","_cell_guid":"da68d3c5-1a4e-4e1b-a929-2bfcca0f4723","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:27:23.284323Z","iopub.execute_input":"2025-07-03T10:27:23.284958Z","iopub.status.idle":"2025-07-03T10:27:23.340815Z","shell.execute_reply.started":"2025-07-03T10:27:23.284933Z","shell.execute_reply":"2025-07-03T10:27:23.339986Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"537f4101-ce51-48d2-9270-601711481f23","_cell_guid":"02823cce-67cb-497d-9a23-b2e107b1f697","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}